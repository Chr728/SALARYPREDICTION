{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebff812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.34.46-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.35.0,>=1.34.46 (from boto3)\n",
      "  Downloading botocore-1.34.46-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/jeremy/anaconda3/lib/python3.11/site-packages (from boto3) (0.10.0)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
      "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/jeremy/anaconda3/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.46->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /home/jeremy/anaconda3/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.46->boto3) (1.26.16)\n",
      "Requirement already satisfied: six>=1.5 in /home/jeremy/anaconda3/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.46->boto3) (1.16.0)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.76\n",
      "    Uninstalling botocore-1.29.76:\n",
      "      Successfully uninstalled botocore-1.29.76\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.5.0 requires botocore<1.29.77,>=1.29.76, but you have botocore 1.34.46 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.34.46 botocore-1.34.46 s3transfer-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100b627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8cb25",
   "metadata": {},
   "source": [
    "# Merge Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209664b5",
   "metadata": {},
   "source": [
    "## Open Connection with Amazon S3 Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a46bb434",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = 'AKIAZQ3DOOYC7J5PI25Z'\n",
    "aws_secret_access_key = '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'\n",
    "\n",
    "bucket_name = 'comp333bucket'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f56be",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e05f6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji_andN(input_string):\n",
    "    result = str(input_string).replace('\\n', ' ')\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F700-\\U0001F77F\"\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', result)\n",
    "\n",
    "def count_skill_keywords(text, keyword_set1, keyword_set2):\n",
    "    text_1 = re.sub(r'[^a-zA-Z\\s]', '', text).lower().split() #for single word\n",
    "    text_2 = re.sub(r'[^a-zA-Z\\s]', '', text).lower() #for word combinations\n",
    "\n",
    "    #for single word\n",
    "    matching_words = set(text_1) & keyword_set1\n",
    "    count_matching_words = len(matching_words)\n",
    "\n",
    "    #for word combinations\n",
    "    keyword_pattern = re.compile('|'.join(re.escape(keyword) for keyword in keyword_set2))\n",
    "    matches = keyword_pattern.findall(text_2)\n",
    "\n",
    "    return len(set(matches)) + count_matching_words\n",
    "\n",
    "CS_keywords = set(['python', 'pytorch', 'sql', 'mxnet', 'mlflow', 'einstein', 'theano', 'pyspark', 'solr', 'mahout',\n",
    " 'cassandra', 'aws', 'powerpoint', 'spark', 'pig', 'sas', 'java', 'nosql', 'docker', 'salesforce', 'scala', 'r',\n",
    " 'c', 'c++', 'net', 'tableau', 'pandas', 'scikitlearn', 'sklearn', 'matlab', 'scala', 'keras', 'tensorflow', 'clojure',\n",
    " 'caffe', 'scipy', 'numpy', 'matplotlib', 'vba', 'spss', 'linux', 'azure', 'cloud', 'gcp', 'mongodb', 'mysql', 'oracle',\n",
    " 'redshift', 'snowflake', 'kafka', 'javascript', 'qlik', 'jupyter', 'perl', 'bigquery', 'unix', 'react',\n",
    " 'scikit', 'powerbi', 's3', 'ec2', 'lambda', 'ssrs', 'kubernetes', 'hana', 'spacy', 'tf', 'django', 'sagemaker',\n",
    " 'seaborn', 'mllib', 'github', 'git', 'elasticsearch', 'splunk', 'airflow', 'looker', 'rapidminer', 'birt', 'pentaho',\n",
    "'jquery', 'nodejs', 'd3', 'plotly', 'bokeh', 'xgboost', 'rstudio', 'shiny', 'dash', 'h20', 'h2o', 'hadoop', 'mapreduce',\n",
    " 'hive', 'cognos', 'angular', 'nltk', 'flask', 'node', 'firebase', 'bigtable', 'rust', 'php', 'cntk', 'lightgbm',\n",
    " 'kubeflow', 'rpython', 'unixlinux', 'postgressql', 'postgresql', 'postgres', 'hbase', 'dask', 'ruby', 'julia', 'tensor',\n",
    " 'dplyr','ggplot2','esquisse','bioconductor','shiny','lubridate','knitr','mlr','quanteda','dt','rcrawler','caret','rmarkdown',\n",
    " 'leaflet','janitor','ggvis','plotly','rcharts','rbokeh','broom','stringr','magrittr','slidify','rvest',\n",
    " 'rmysql','rsqlite','prophet','glmnet','text2vec','snowballc','quantmod','rstan','swirl','datasciencer', 'cleansing', 'chatbot', 'cleaning', 'blockchain', 'causality', 'correlation', 'bandit', 'anomaly', 'kpi',\n",
    " 'dashboard', 'geospatial', 'ocr',  'pca', 'gis', 'svm', 'svd', 'tuning', 'hyperparameter', 'hypothesis',\n",
    " 'salesforcecom', 'segmentation', 'biostatistics', 'unsupervised', 'supervised', 'exploratory',\n",
    " 'recommender', 'recommendations', 'research', 'sequencing', 'probability', 'reinforcement', 'graph', 'bioinformatics',\n",
    "  'knn', 'etl', 'normalization', 'classification', 'optimizing', 'prediction', 'forecasting',\n",
    " 'clustering', 'optimization', 'visualization', 'nlp', 'c#',\n",
    " 'regression', 'logistic', 'cnn', 'glm','rnn', 'lstm', 'gbm', 'boosting', 'recurrent', 'convolutional', 'bayesian',\n",
    " 'bayes'])\n",
    "\n",
    "group_CS_keywords = set(['amazon web services', 'google cloud', 'sql server', 'random forest', 'natural language processing', 'machine learning', 'decision tree', 'deep learning', 'experimental design',\n",
    " 'time series', 'nearest neighbors', 'neural network', 'support vector machine', 'computer vision', 'machine vision', 'dimensionality reduction',\n",
    " 'text analytics', 'power bi', 'a/b testing', 'ab testing', 'chat bot', 'data mining'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce631aa",
   "metadata": {},
   "source": [
    "## Jobspikr Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec56ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file key for the CSV file in AWS S3\n",
    "file_key = 'data_scientist_united_states_job_postings_jobspikr.csv'\n",
    "\n",
    "# Retrieve the object from S3 bucket using the specified file key\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "# Read the content of the object\n",
    "content = obj['Body'].read()\n",
    "\n",
    "# Create a Pandas DataFrame from the CSV content\n",
    "Jobspikr = pd.read_csv(BytesIO(content), engine='python')\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "Jobspikr = Jobspikr.dropna(subset=['salary_offered', 'city', 'state'])\n",
    "\n",
    "# Filter by job type to include only 'Full Time' positions\n",
    "Jobspikr = Jobspikr[Jobspikr['job_type'] == 'Full Time']\n",
    "\n",
    "# Select relevant columns for further analysis\n",
    "Jobspikr = Jobspikr[['job_title', 'job_description', 'company_name', 'city', 'state', 'post_date', 'salary_offered']]\n",
    "\n",
    "# Define a regular expression pattern for extracting salary information\n",
    "salary_pattern = r'\\$([\\d,]+)[Kk]?\\s*-\\s*\\$([\\d,]+)[Kk]?'\n",
    "\n",
    "# Function to extract and preprocess salary information from a string\n",
    "def process_salary_string(salary_str):\n",
    "    # Extract salary information using the defined pattern\n",
    "    match = re.search(salary_pattern, salary_str)\n",
    "    if match:\n",
    "        lower_salary, upper_salary = map(lambda x: float(x.replace(',', '')), match.groups())\n",
    "    else:\n",
    "        # If the pattern does not match, extract numeric values using a different approach\n",
    "        numeric_values = [float(val.replace(',', '')) for val in re.findall(r'\\b[\\d,]+\\b', salary_str)][:2]\n",
    "        lower_salary, upper_salary = numeric_values if len(numeric_values) == 2 else (None, None)\n",
    "    return lower_salary, upper_salary\n",
    "\n",
    "# Apply the salary extraction function to the 'salary_offered' column\n",
    "Jobspikr['lower_salary'], Jobspikr['upper_salary'] = zip(*Jobspikr['salary_offered'].map(process_salary_string))\n",
    "\n",
    "# Create a new column 'approximated_salary' by taking the mean of lower and upper salary values\n",
    "Jobspikr['approximated_salary'] = Jobspikr[['lower_salary', 'upper_salary']].mean(axis=1).apply(lambda x: int(str(x).split('.')[0][:3]) if pd.notna(x) else x)\n",
    "\n",
    "# Drop unnecessary columns related to salary\n",
    "Jobspikr = Jobspikr.drop(columns=['salary_offered', 'lower_salary', 'upper_salary'])\n",
    "\n",
    "# Drop rows with missing values in the new 'approximated_salary' column\n",
    "Jobspikr = Jobspikr.dropna(subset=['approximated_salary'])\n",
    "\n",
    "# Convert 'post_date' to datetime format for further analysis\n",
    "Jobspikr['post_date'] = pd.to_datetime(Jobspikr['post_date'])\n",
    "\n",
    "# Extract year, month, and day from 'post_date' into separate columns\n",
    "Jobspikr[['year', 'month', 'day']] = Jobspikr['post_date'].apply(lambda x: pd.Series([x.year, x.month, x.day]))\n",
    "\n",
    "# Drop the original 'post_date' column\n",
    "Jobspikr = Jobspikr.drop(columns=['post_date'])\n",
    "\n",
    "# Rename columns for clarity and consistency\n",
    "Jobspikr = Jobspikr.rename(columns={\n",
    "    'job_title': 'Title',\n",
    "    'job_description': 'Description',\n",
    "    'company_name': 'Company Name',\n",
    "    'city': 'City',\n",
    "    'state': 'State',\n",
    "    'year': 'Year',\n",
    "    'month': 'Month',\n",
    "    'day': 'Day',\n",
    "    'approximated_salary': 'Salary'\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Round and adjust the 'Salary' column to represent salaries in thousands\n",
    "Jobspikr['Salary'] = (Jobspikr['Salary'] * 1000).round()\n",
    "\n",
    "# Combine 'Title' and 'Description' columns into a new 'description' column\n",
    "# This will be useful later\n",
    "Jobspikr['description'] = Jobspikr['Title'] + ' ' + Jobspikr['Description']\n",
    "Jobspikr = Jobspikr.drop(['Description'], axis=1)\n",
    "\n",
    "# Remove emojis and newline characters from the 'description' column\n",
    "Jobspikr['description'] = Jobspikr['description'].map(remove_emoji_andN)\n",
    "\n",
    "# Count skill and tool keywords in the 'description' column and create new columns for them\n",
    "Jobspikr['CS_keywords'] = Jobspikr.apply(lambda row: count_skill_keywords(row['description'], CS_keywords, group_CS_keywords), axis=1)\n",
    "\n",
    "# Select and order the final columns for the DataFrame\n",
    "Jobspikr = Jobspikr[['Title', 'description', 'Company Name', 'City', 'State', 'Salary', 'Year', 'Month', 'Day', 'CS_keywords']]\n",
    "\n",
    "# Rename the 'description' column for consistency\n",
    "fix_description_column_name= {\n",
    "    'description': 'Description',\n",
    "}\n",
    "Jobspikr = Jobspikr.rename(columns=fix_description_column_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2a13ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>CS_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>90000</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Carolinas HealthCare System</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1  Senior Data Scientist - Infectious Disease and...   \n",
       "2  Senior Data Scientist, Advanced Marketing Anal...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1  Senior Data Scientist - Infectious Disease and...   \n",
       "2  Senior Data Scientist, Advanced Marketing Anal...   \n",
       "\n",
       "                  Company Name       City State  Salary  Year  Month  Day  \\\n",
       "0               Morgan Stanley   New York    NY   90000  2019      8   20   \n",
       "1  Carolinas HealthCare System  Charlotte    NC  125000  2019      9    6   \n",
       "2                      Spotify   New York    NY  125000  2019      8   23   \n",
       "\n",
       "   CS_keywords  \n",
       "0            2  \n",
       "1            5  \n",
       "2           11  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jobspikr.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba26995",
   "metadata": {},
   "source": [
    "## Glassdoor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc51ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'Data_Science_Job_Posting_on_Glassdoor.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = obj['Body'].read()\n",
    "Glassdoor = pd.read_csv(BytesIO(content), engine='python')\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "Glassdoor = Glassdoor.dropna(subset=['Salary Estimate', 'Location', 'Company Name'])\n",
    "\n",
    "# Define a regular expression pattern for salary extraction\n",
    "salary_pattern = r'\\$([\\d,]+)[Kk]?\\s*-\\s*\\$([\\d,]+)[Kk]?'\n",
    "\n",
    "# Extract and preprocess salaries\n",
    "def process_salary_string(salary_str):\n",
    "    match = re.search(salary_pattern, salary_str)\n",
    "    if match:\n",
    "        lower_salary, upper_salary = map(lambda x: float(x.replace(',', '')), match.groups())\n",
    "    else:\n",
    "        numeric_values = [float(val.replace(',', '')) for val in re.findall(r'\\b[\\d,]+\\b', salary_str)][:2]\n",
    "        lower_salary, upper_salary = numeric_values if len(numeric_values) == 2 else (None, None)\n",
    "    return lower_salary, upper_salary\n",
    "\n",
    "# Apply the function to extract and preprocess salaries\n",
    "Glassdoor['lower_salary'], Glassdoor['upper_salary'] = zip(*Glassdoor['Salary Estimate'].map(process_salary_string))\n",
    "\n",
    "# Create a new column 'approximated_salary'\n",
    "Glassdoor['approximated_salary'] = Glassdoor[['lower_salary', 'upper_salary']].mean(axis=1).apply(lambda x: int(str(x).split('.')[0][:3]) if pd.notna(x) else x)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "Glassdoor = Glassdoor.drop(columns=['Salary Estimate', 'lower_salary', 'upper_salary'])\n",
    "\n",
    "# Drop rows with missing values in the new column\n",
    "Glassdoor = Glassdoor.dropna(subset=['approximated_salary'])\n",
    "\n",
    "# Reset the index to have consecutive indices\n",
    "Glassdoor = Glassdoor.reset_index(drop=True)\n",
    "\n",
    "# Select relevant columns\n",
    "Glassdoor = Glassdoor[['Job Title', 'Job Description', 'Company Name', 'Location','approximated_salary']]\n",
    "\n",
    "# Split 'Location' into 'City' and 'State' columns\n",
    "location_split = Glassdoor['Location'].str.split(', ', expand=True)\n",
    "Glassdoor['City'] = location_split[0]\n",
    "Glassdoor['State'] = location_split[1]\n",
    "Glassdoor = Glassdoor.dropna(subset=['State'])\n",
    "\n",
    "# Drop the original 'Location' column\n",
    "Glassdoor = Glassdoor.drop(columns=['Location'])\n",
    "\n",
    "# Rename columns\n",
    "Glassdoor = Glassdoor.rename(columns={\n",
    "    'Job Title': 'Title',\n",
    "    'Job Description': 'Description',\n",
    "    'city': 'City',\n",
    "    'state': 'State',\n",
    "    'year': 'Year',\n",
    "    'approximated_salary': 'Salary'\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Add columns filled with NaN\n",
    "Glassdoor['Year'] = np.nan\n",
    "Glassdoor['Month'] = np.nan\n",
    "Glassdoor['Day'] = np.nan\n",
    "\n",
    "# Convert columns to Int64 type\n",
    "Glassdoor['Year'] = Glassdoor['Year'].astype('Int64')\n",
    "Glassdoor['Month'] = Glassdoor['Month'].astype('Int64')\n",
    "Glassdoor['Day'] = Glassdoor['Day'].astype('Int64')\n",
    "Glassdoor['Salary'] = Glassdoor['Salary'].astype('Int64')\n",
    "\n",
    "# Reorder columns to match the specified order\n",
    "column_order = ['Title', 'Description', 'Company Name', 'City', 'State', 'Salary', 'Year', 'Month', 'Day']\n",
    "Glassdoor = Glassdoor[column_order]\n",
    "Glassdoor['Salary'] = Glassdoor['Salary'].apply(lambda x: x*1000)\n",
    "\n",
    "Glassdoor['description'] = Glassdoor['Title'] + ' ' + Glassdoor['Description']\n",
    "Glassdoor = Glassdoor.drop(['Description'], axis=1)\n",
    "\n",
    "Glassdoor['Company Name'] = Glassdoor['Company Name'].map(lambda x: x.split('\\n')[0])\n",
    "\n",
    "Glassdoor['description'] = Glassdoor['description'].map(remove_emoji_andN)\n",
    "\n",
    "Glassdoor['CS_keywords'] = Glassdoor.apply(lambda row: count_skill_keywords(row['description'], CS_keywords, group_CS_keywords), axis=1)\n",
    "\n",
    "Glassdoor = Glassdoor[['Title', 'description', 'Company Name', 'City', 'State', 'Salary', 'Year', 'Month', 'Day', 'CS_keywords']]\n",
    "fix_description_column_name= {\n",
    "    'description': 'Description',\n",
    "}\n",
    "Glassdoor = Glassdoor.rename(columns=fix_description_column_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f381d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>CS_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr Data Scientist</td>\n",
       "      <td>Sr Data Scientist Description  The Senior Data...</td>\n",
       "      <td>Healthfirst</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>154000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist Secure our Nation, Ignite your ...</td>\n",
       "      <td>ManTech</td>\n",
       "      <td>Chantilly</td>\n",
       "      <td>VA</td>\n",
       "      <td>154000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Data Scientist Overview   Analysis Group is on...</td>\n",
       "      <td>Analysis Group</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>154000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Title                                        Description  \\\n",
       "0  Sr Data Scientist  Sr Data Scientist Description  The Senior Data...   \n",
       "1     Data Scientist  Data Scientist Secure our Nation, Ignite your ...   \n",
       "2     Data Scientist  Data Scientist Overview   Analysis Group is on...   \n",
       "\n",
       "     Company Name       City State  Salary  Year  Month   Day  CS_keywords  \n",
       "0     Healthfirst   New York    NY  154000  <NA>   <NA>  <NA>           10  \n",
       "1         ManTech  Chantilly    VA  154000  <NA>   <NA>  <NA>            8  \n",
       "2  Analysis Group     Boston    MA  154000  <NA>   <NA>  <NA>           13  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Glassdoor.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8774505a",
   "metadata": {},
   "source": [
    "## Indeed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de2e39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'final_ai_jobs.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = obj['Body'].read()\n",
    "AI_jobs = pd.read_csv(BytesIO(content), engine='python')\n",
    "\n",
    "# Drop rows with missing values in specified columns\n",
    "AI_jobs = AI_jobs[['job_title', 'job_description', 'company_name', 'company_location', 'salary']]\n",
    "\n",
    "# Split 'Location' into 'City' and 'State' columns\n",
    "location_split = AI_jobs['company_location'].str.split(', ', expand=True)\n",
    "AI_jobs['City'] = location_split[0]\n",
    "AI_jobs['State'] = location_split[1]\n",
    "AI_jobs = AI_jobs.dropna(subset=['State'])\n",
    "\n",
    "# Drop the original 'Location' column\n",
    "AI_jobs = AI_jobs.drop(columns=['company_location'])\n",
    "\n",
    "#AI_jobs = AI_jobs['salary'] / 100000\n",
    "\n",
    "# Rename columns\n",
    "AI_jobs = AI_jobs.rename(columns={\n",
    "    'job_title': 'Title',\n",
    "    'job_description': 'Description',\n",
    "    'company_name': 'Company Name',\n",
    "    'salary': 'Salary'\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Add columns filled with NaN\n",
    "AI_jobs['Year'] = np.nan\n",
    "AI_jobs['Month'] = np.nan\n",
    "AI_jobs['Day'] = np.nan\n",
    "\n",
    "# Remove rows where the salary is 'Not available'\n",
    "AI_jobs = AI_jobs[AI_jobs['Salary'] != 'Not available']\n",
    "\n",
    "\n",
    "# Convert columns to Int64 type\n",
    "AI_jobs['Year'] = AI_jobs['Year'].astype('Int64')\n",
    "AI_jobs['Month'] = AI_jobs['Month'].astype('Int64')\n",
    "AI_jobs['Day'] = AI_jobs['Day'].astype('Int64')\n",
    "AI_jobs['Salary'] = AI_jobs['Salary'].astype('float32').fillna(0).astype('int64')\n",
    "\n",
    "AI_jobs['description'] = AI_jobs['Title'] + ' ' + AI_jobs['Description']\n",
    "AI_jobs = AI_jobs.drop(['Description'], axis=1)\n",
    "\n",
    "AI_jobs['description'] = AI_jobs['description'].map(remove_emoji_andN)\n",
    "\n",
    "AI_jobs['CS_keywords'] = AI_jobs.apply(lambda row: count_skill_keywords(row['description'], CS_keywords, group_CS_keywords), axis=1)\n",
    "\n",
    "AI_jobs = AI_jobs[['Title', 'description', 'Company Name', 'City', 'State', 'Salary', 'Year', 'Month', 'Day', 'CS_keywords']]\n",
    "fix_description_column_name= {\n",
    "    'description': 'Description',\n",
    "}\n",
    "\n",
    "AI_jobs = AI_jobs.rename(columns=fix_description_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84bbadb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>CS_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UX Researcher, Qualitative - Generative AI</td>\n",
       "      <td>UX Researcher, Qualitative - Generative AI Wor...</td>\n",
       "      <td>Meta</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>184500</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fundamental Language Research Scientist - Gene...</td>\n",
       "      <td>Fundamental Language Research Scientist - Gene...</td>\n",
       "      <td>Meta</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>173500</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist, Product - Generative AI</td>\n",
       "      <td>Data Scientist, Product - Generative AI You wi...</td>\n",
       "      <td>Facebook App</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>164000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0         UX Researcher, Qualitative - Generative AI   \n",
       "1  Fundamental Language Research Scientist - Gene...   \n",
       "2            Data Scientist, Product - Generative AI   \n",
       "\n",
       "                                         Description  Company Name      City  \\\n",
       "0  UX Researcher, Qualitative - Generative AI Wor...          Meta  New York   \n",
       "1  Fundamental Language Research Scientist - Gene...          Meta  New York   \n",
       "2  Data Scientist, Product - Generative AI You wi...  Facebook App  New York   \n",
       "\n",
       "  State  Salary  Year  Month   Day  CS_keywords  \n",
       "0    NY  184500  <NA>   <NA>  <NA>            1  \n",
       "1    NY  173500  <NA>   <NA>  <NA>            2  \n",
       "2    NY  164000  <NA>   <NA>  <NA>            1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_jobs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f401fba",
   "metadata": {},
   "source": [
    "## LinkedIn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f5ea0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'LinkedIn Job Postings.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = obj['Body'].read()\n",
    "LinkedIn  = pd.read_csv(BytesIO(content), engine='python')\n",
    "\n",
    "file_key = 'LinkedIn Job Postings_companies.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = obj['Body'].read()\n",
    "company_id  = pd.read_csv(BytesIO(content), engine='python')\n",
    "\n",
    "LinkedIn = pd.merge(LinkedIn , company_id, on='company_id', how='inner')\n",
    "\n",
    "# Filter by job type\n",
    "LinkedIn = LinkedIn[LinkedIn['formatted_work_type'] == 'Full-time']\n",
    "LinkedIn = LinkedIn[LinkedIn['work_type'] == 'FULL_TIME']\n",
    "\n",
    "def convert_to_yearly(row):\n",
    "    if row['pay_period'] == 'MONTHLY':\n",
    "        return row['med_salary'] * 12\n",
    "    elif row['pay_period'] == 'HOURLY':\n",
    "        # Assuming 40 hours per week and 52 weeks per year\n",
    "        return row['med_salary'] * 40 * 52\n",
    "    else:\n",
    "        return row['med_salary']\n",
    "\n",
    "# Use the conversion function to create a new 'med_salary' column\n",
    "LinkedIn['med_salary'] = LinkedIn.apply(convert_to_yearly, axis=1)\n",
    "\n",
    "LinkedIn = LinkedIn[['title', 'description_x', 'description_y', 'name', 'city', 'min_salary', 'med_salary', 'max_salary', 'state', 'original_listed_time']]\n",
    "            \n",
    "LinkedIn['description'] = LinkedIn['description_x'] + ' ' +LinkedIn['description_y'] + ' ' + LinkedIn['title']\n",
    "\n",
    "LinkedIn = LinkedIn.drop(['description_y', 'description_x'], axis=1)\n",
    "\n",
    "LinkedIn['description'] = LinkedIn['description'].map(remove_emoji_andN)\n",
    "\n",
    "LinkedIn['CS_keywords'] = LinkedIn.apply(lambda row: count_skill_keywords(row['description'], CS_keywords, group_CS_keywords), axis=1)\n",
    "\n",
    "LinkedIn = LinkedIn[['title','description', 'name', 'city','state' ,'min_salary', 'med_salary', 'max_salary', 'original_listed_time','CS_keywords']]\n",
    "\n",
    "fix_description_column_name = {\n",
    "    'title': 'Title',\n",
    "    'description': 'Description',\n",
    "    'name': 'Company Name',\n",
    "    'city': 'City',\n",
    "    'state': 'State',\n",
    "    'med_salary': 'Salary'\n",
    "}\n",
    "\n",
    "LinkedIn = LinkedIn.rename(columns=fix_description_column_name)\n",
    "\n",
    "LinkedIn = LinkedIn.drop(['min_salary', 'max_salary', 'original_listed_time'], axis=1)\n",
    "\n",
    "# Add new Year, Month, and Day columns\n",
    "LinkedIn['Year'] = np.nan  \n",
    "LinkedIn['Month'] = np.nan  \n",
    "LinkedIn['Day'] = np.nan  \n",
    "\n",
    "# Reorder columns\n",
    "LinkedIn = LinkedIn[['Title', 'Description', 'Company Name', 'City', 'State', 'Salary','Year', 'Month', 'Day', 'CS_keywords']]\n",
    "\n",
    "# Drop rows that has either null, zero or empty in specified columns\n",
    "LinkedIn = LinkedIn.dropna(subset=['Description', 'Salary', 'City', 'State'])\n",
    "LinkedIn = LinkedIn[(LinkedIn['Description'] != '') & (LinkedIn['Salary'] != 0) & (LinkedIn['City'] != '')  & (LinkedIn['City'] != '0') &  (LinkedIn['State'] != '') & (LinkedIn['State'] != '0')]\n",
    "\n",
    "LinkedIn['Salary'] = LinkedIn['Salary'].astype('float32').fillna(0).astype('int64')\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "LinkedIn.to_csv('cleaned_LinkedIn.csv', index=False)\n",
    "\n",
    "#Keep only job posts related to CS (at least 3 keywords)\n",
    "LinkedIn = LinkedIn[LinkedIn[\"CS_keywords\"] > 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bac3b5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>CS_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>Senior Communications Manager, Texas McCombs</td>\n",
       "      <td>Job Posting Title:  Senior Communications Mana...</td>\n",
       "      <td>The University of Texas at Austin</td>\n",
       "      <td>Austin</td>\n",
       "      <td>tx</td>\n",
       "      <td>80000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>Front End Web Developer</td>\n",
       "      <td>About Boomi And What Makes Us Special  Are you...</td>\n",
       "      <td>Boomi</td>\n",
       "      <td>Chesterbrook</td>\n",
       "      <td>PA</td>\n",
       "      <td>82366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>Python Developer</td>\n",
       "      <td>ECS is seeking a Python Developer  for our USP...</td>\n",
       "      <td>ECS</td>\n",
       "      <td>Fairfax</td>\n",
       "      <td>VA</td>\n",
       "      <td>85000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>Data Engineer USA citizenship required REMOTE</td>\n",
       "      <td>Required United States Citizenship • Hands On ...</td>\n",
       "      <td>TechPerm Incorporated</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>125000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>Budget Analyst</td>\n",
       "      <td>Job Position:-  Budget Analyst Client Name:- S...</td>\n",
       "      <td>iAppsData Inc.</td>\n",
       "      <td>Jersey City</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>104000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6824</th>\n",
       "      <td>Azure/Databricks Data Engineer - Empower (remo...</td>\n",
       "      <td>Company Description  Company Overview  Hitachi...</td>\n",
       "      <td>Hitachi Solutions America</td>\n",
       "      <td>Irvine</td>\n",
       "      <td>CA</td>\n",
       "      <td>175000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8629</th>\n",
       "      <td>ERP Systems Administrator</td>\n",
       "      <td>About Senox:Senox Corporation is a leading man...</td>\n",
       "      <td>Senox Corporation</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "      <td>104650</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10260</th>\n",
       "      <td>Systems Developer</td>\n",
       "      <td>Job Name: OR - Systems Developer 3  Facility A...</td>\n",
       "      <td>TriOptus</td>\n",
       "      <td>Clarksburg</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>108160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10333</th>\n",
       "      <td>Security and Systems Manager</td>\n",
       "      <td>Location: 13001 Starkey Road, Largo, FL 33773 ...</td>\n",
       "      <td>Pinellas County Government</td>\n",
       "      <td>Clearwater</td>\n",
       "      <td>Florida</td>\n",
       "      <td>80000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11414</th>\n",
       "      <td>Senior Director of Technology</td>\n",
       "      <td>Description  7+ years out of university with 5...</td>\n",
       "      <td>Resolute Consulting Group</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Texas</td>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13404</th>\n",
       "      <td>Sr. Software Engineer - Data Platform</td>\n",
       "      <td>About Pinecone  Pinecone is pioneering a vecto...</td>\n",
       "      <td>Pinecone</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>125000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13405</th>\n",
       "      <td>Sr. Fullstack Software Engineer</td>\n",
       "      <td>About Pinecone  Pinecone is pioneering a vecto...</td>\n",
       "      <td>Pinecone</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>125000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13406</th>\n",
       "      <td>Sr. Software Engineer - Data Platform</td>\n",
       "      <td>About Pinecone  Pinecone is pioneering a vecto...</td>\n",
       "      <td>Pinecone</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>125000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13407</th>\n",
       "      <td>Sr. Software Engineer - Data Platform</td>\n",
       "      <td>About Pinecone  Pinecone is pioneering a vecto...</td>\n",
       "      <td>Pinecone</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>125000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14347</th>\n",
       "      <td>Business Intelligence Analyst Intern</td>\n",
       "      <td>R375941  Comcast brings together the best in m...</td>\n",
       "      <td>Comcast</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>52000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15390</th>\n",
       "      <td>Hardware Engineer</td>\n",
       "      <td>Automation of Test process and latest certific...</td>\n",
       "      <td>LTIMindtree</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>MH</td>\n",
       "      <td>80000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16659</th>\n",
       "      <td>Senior Security Engineer</td>\n",
       "      <td>Senior Security EngineerLocation: hybrid, Nort...</td>\n",
       "      <td>Vaco</td>\n",
       "      <td>Brentwood</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>135000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18346</th>\n",
       "      <td>Sr. Machine Learning Engineer</td>\n",
       "      <td>Summary  JOB DESCRIPTION  At Levi Strauss &amp; Co...</td>\n",
       "      <td>Levi Strauss &amp; Co.</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>122000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22390</th>\n",
       "      <td>Software Solutions Architect</td>\n",
       "      <td>Overview  Systems Planning and Analysis, Inc. ...</td>\n",
       "      <td>Systems Planning and Analysis, Inc.</td>\n",
       "      <td>Alexandria</td>\n",
       "      <td>VA</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22394</th>\n",
       "      <td>Data Analytics Analyst</td>\n",
       "      <td>Overview  Systems Planning and Analysis, Inc. ...</td>\n",
       "      <td>Systems Planning and Analysis, Inc.</td>\n",
       "      <td>Alexandria</td>\n",
       "      <td>VA</td>\n",
       "      <td>130000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22981</th>\n",
       "      <td>2024 R&amp;D Summer Intern- Track 2</td>\n",
       "      <td>About Alcon: As the global leader in eye care,...</td>\n",
       "      <td>Alcon</td>\n",
       "      <td>Geneva</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>93600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22983</th>\n",
       "      <td>2024 R&amp;D Summer Intern- Track 2</td>\n",
       "      <td>About Alcon: As the global leader in eye care,...</td>\n",
       "      <td>Alcon</td>\n",
       "      <td>Geneva</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>93600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22985</th>\n",
       "      <td>2024 R&amp;D Summer Intern- Track 2</td>\n",
       "      <td>About Alcon: As the global leader in eye care,...</td>\n",
       "      <td>Alcon</td>\n",
       "      <td>Geneva</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>93600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22986</th>\n",
       "      <td>2024 R&amp;D Summer Intern- Track 2</td>\n",
       "      <td>About Alcon: As the global leader in eye care,...</td>\n",
       "      <td>Alcon</td>\n",
       "      <td>Geneva</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>93600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22991</th>\n",
       "      <td>2024 R&amp;D Summer Intern- Track 2</td>\n",
       "      <td>About Alcon: As the global leader in eye care,...</td>\n",
       "      <td>Alcon</td>\n",
       "      <td>Geneva</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>93600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23620</th>\n",
       "      <td>Full Stack Developer</td>\n",
       "      <td>Piper Companies is looking for a Sr. Full Stac...</td>\n",
       "      <td>Piper Companies</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>156000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23625</th>\n",
       "      <td>Senior Cloud Architect</td>\n",
       "      <td>Piper Companies is seeking an Senior Cloud Arc...</td>\n",
       "      <td>Piper Companies</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23633</th>\n",
       "      <td>Senior Cloud Architect</td>\n",
       "      <td>Piper Companies is seeking an Senior Cloud Arc...</td>\n",
       "      <td>Piper Companies</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23643</th>\n",
       "      <td>Data Analytics Solutions Engineer</td>\n",
       "      <td>Piper Companies is looking for a Data Analytic...</td>\n",
       "      <td>Piper Companies</td>\n",
       "      <td>McLean</td>\n",
       "      <td>VA</td>\n",
       "      <td>125000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24011</th>\n",
       "      <td>Business Intelligence Consulting Intern - Summ...</td>\n",
       "      <td>We are the leading provider of professional se...</td>\n",
       "      <td>RSM US LLP</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>68640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24371</th>\n",
       "      <td>Program Manager, Computer Science and Artifici...</td>\n",
       "      <td>The School of Engineering   Stanford Engineeri...</td>\n",
       "      <td>Stanford University</td>\n",
       "      <td>Stanford</td>\n",
       "      <td>CA</td>\n",
       "      <td>79000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24568</th>\n",
       "      <td>Application Support Analyst</td>\n",
       "      <td>Application Support Analyst-Pay: $24.64 (non-e...</td>\n",
       "      <td>Prosum</td>\n",
       "      <td>El Segundo</td>\n",
       "      <td>California</td>\n",
       "      <td>51251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24641</th>\n",
       "      <td>Vendor Experience Business Analyst</td>\n",
       "      <td>Pax8 is the leading cloud-based technology mar...</td>\n",
       "      <td>Pax8</td>\n",
       "      <td>Greenwood Village</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>91000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24644</th>\n",
       "      <td>Technical Support Engineer III- Infrastructure</td>\n",
       "      <td>Pax8 is the leading value-added cloud-based Sa...</td>\n",
       "      <td>Pax8</td>\n",
       "      <td>Greenwood Village</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>74000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25142</th>\n",
       "      <td>Sales Operations Senior Manager</td>\n",
       "      <td>At Entertainment Partners we help to power Osc...</td>\n",
       "      <td>Entertainment Partners</td>\n",
       "      <td>Burbank</td>\n",
       "      <td>CA</td>\n",
       "      <td>130000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28244</th>\n",
       "      <td>Application Developer</td>\n",
       "      <td>When joining Elevations, you can expect to wor...</td>\n",
       "      <td>Elevations Credit Union</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>110000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28794</th>\n",
       "      <td>Metal Additive Manufacturing - Postdoctoral Re...</td>\n",
       "      <td>Company Description Join us and make YOUR mark...</td>\n",
       "      <td>Lawrence Livermore National Laboratory</td>\n",
       "      <td>Livermore</td>\n",
       "      <td>CA</td>\n",
       "      <td>108840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30410</th>\n",
       "      <td>Epidemiology Investigator 2 (Hybrid Eligible)</td>\n",
       "      <td>About Us Our mission is to save lives, reduce ...</td>\n",
       "      <td>Ohio Department of Public Safety</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>57907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32028</th>\n",
       "      <td>Wholesale Business Analyst - Montvale, NJ (Hyb...</td>\n",
       "      <td>Wholesale Business Analyst - Montvale, NJ (Hyb...</td>\n",
       "      <td>Flight Centre Travel Group, The Americas</td>\n",
       "      <td>Montvale</td>\n",
       "      <td>NJ</td>\n",
       "      <td>75000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32055</th>\n",
       "      <td>AEM Developer</td>\n",
       "      <td>Senior AEM Developer | Mountain view, CA | Pho...</td>\n",
       "      <td>RD SOLUTIONS INC</td>\n",
       "      <td>Iselin</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>135200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32095</th>\n",
       "      <td>MBA Fall Intern, Insights</td>\n",
       "      <td>ABOUT ELEVATE SPORTS VENTURES:Elevate Sports V...</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>52000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32096</th>\n",
       "      <td>Fall Undergraduate Intern, Insights</td>\n",
       "      <td>ABOUT ELEVATE SPORTS VENTURES: Elevate Sports ...</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>41600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "1593        Senior Communications Manager, Texas McCombs   \n",
       "1812                             Front End Web Developer   \n",
       "2236                                    Python Developer   \n",
       "4589       Data Engineer USA citizenship required REMOTE   \n",
       "5787                                      Budget Analyst   \n",
       "6824   Azure/Databricks Data Engineer - Empower (remo...   \n",
       "8629                           ERP Systems Administrator   \n",
       "10260                                  Systems Developer   \n",
       "10333                       Security and Systems Manager   \n",
       "11414                      Senior Director of Technology   \n",
       "13404              Sr. Software Engineer - Data Platform   \n",
       "13405                    Sr. Fullstack Software Engineer   \n",
       "13406              Sr. Software Engineer - Data Platform   \n",
       "13407              Sr. Software Engineer - Data Platform   \n",
       "14347               Business Intelligence Analyst Intern   \n",
       "15390                                  Hardware Engineer   \n",
       "16659                           Senior Security Engineer   \n",
       "18346                      Sr. Machine Learning Engineer   \n",
       "22390                       Software Solutions Architect   \n",
       "22394                             Data Analytics Analyst   \n",
       "22981                    2024 R&D Summer Intern- Track 2   \n",
       "22983                    2024 R&D Summer Intern- Track 2   \n",
       "22985                    2024 R&D Summer Intern- Track 2   \n",
       "22986                    2024 R&D Summer Intern- Track 2   \n",
       "22991                    2024 R&D Summer Intern- Track 2   \n",
       "23620                               Full Stack Developer   \n",
       "23625                             Senior Cloud Architect   \n",
       "23633                             Senior Cloud Architect   \n",
       "23643                  Data Analytics Solutions Engineer   \n",
       "24011  Business Intelligence Consulting Intern - Summ...   \n",
       "24371  Program Manager, Computer Science and Artifici...   \n",
       "24568                        Application Support Analyst   \n",
       "24641                 Vendor Experience Business Analyst   \n",
       "24644     Technical Support Engineer III- Infrastructure   \n",
       "25142                    Sales Operations Senior Manager   \n",
       "28244                              Application Developer   \n",
       "28794  Metal Additive Manufacturing - Postdoctoral Re...   \n",
       "30410      Epidemiology Investigator 2 (Hybrid Eligible)   \n",
       "32028  Wholesale Business Analyst - Montvale, NJ (Hyb...   \n",
       "32055                                      AEM Developer   \n",
       "32095                          MBA Fall Intern, Insights   \n",
       "32096                Fall Undergraduate Intern, Insights   \n",
       "\n",
       "                                             Description  \\\n",
       "1593   Job Posting Title:  Senior Communications Mana...   \n",
       "1812   About Boomi And What Makes Us Special  Are you...   \n",
       "2236   ECS is seeking a Python Developer  for our USP...   \n",
       "4589   Required United States Citizenship • Hands On ...   \n",
       "5787   Job Position:-  Budget Analyst Client Name:- S...   \n",
       "6824   Company Description  Company Overview  Hitachi...   \n",
       "8629   About Senox:Senox Corporation is a leading man...   \n",
       "10260  Job Name: OR - Systems Developer 3  Facility A...   \n",
       "10333  Location: 13001 Starkey Road, Largo, FL 33773 ...   \n",
       "11414  Description  7+ years out of university with 5...   \n",
       "13404  About Pinecone  Pinecone is pioneering a vecto...   \n",
       "13405  About Pinecone  Pinecone is pioneering a vecto...   \n",
       "13406  About Pinecone  Pinecone is pioneering a vecto...   \n",
       "13407  About Pinecone  Pinecone is pioneering a vecto...   \n",
       "14347  R375941  Comcast brings together the best in m...   \n",
       "15390  Automation of Test process and latest certific...   \n",
       "16659  Senior Security EngineerLocation: hybrid, Nort...   \n",
       "18346  Summary  JOB DESCRIPTION  At Levi Strauss & Co...   \n",
       "22390  Overview  Systems Planning and Analysis, Inc. ...   \n",
       "22394  Overview  Systems Planning and Analysis, Inc. ...   \n",
       "22981  About Alcon: As the global leader in eye care,...   \n",
       "22983  About Alcon: As the global leader in eye care,...   \n",
       "22985  About Alcon: As the global leader in eye care,...   \n",
       "22986  About Alcon: As the global leader in eye care,...   \n",
       "22991  About Alcon: As the global leader in eye care,...   \n",
       "23620  Piper Companies is looking for a Sr. Full Stac...   \n",
       "23625  Piper Companies is seeking an Senior Cloud Arc...   \n",
       "23633  Piper Companies is seeking an Senior Cloud Arc...   \n",
       "23643  Piper Companies is looking for a Data Analytic...   \n",
       "24011  We are the leading provider of professional se...   \n",
       "24371  The School of Engineering   Stanford Engineeri...   \n",
       "24568  Application Support Analyst-Pay: $24.64 (non-e...   \n",
       "24641  Pax8 is the leading cloud-based technology mar...   \n",
       "24644  Pax8 is the leading value-added cloud-based Sa...   \n",
       "25142  At Entertainment Partners we help to power Osc...   \n",
       "28244  When joining Elevations, you can expect to wor...   \n",
       "28794  Company Description Join us and make YOUR mark...   \n",
       "30410  About Us Our mission is to save lives, reduce ...   \n",
       "32028  Wholesale Business Analyst - Montvale, NJ (Hyb...   \n",
       "32055  Senior AEM Developer | Mountain view, CA | Pho...   \n",
       "32095  ABOUT ELEVATE SPORTS VENTURES:Elevate Sports V...   \n",
       "32096  ABOUT ELEVATE SPORTS VENTURES: Elevate Sports ...   \n",
       "\n",
       "                                   Company Name               City  \\\n",
       "1593          The University of Texas at Austin             Austin   \n",
       "1812                                      Boomi       Chesterbrook   \n",
       "2236                                        ECS            Fairfax   \n",
       "4589                      TechPerm Incorporated          Annapolis   \n",
       "5787                             iAppsData Inc.        Jersey City   \n",
       "6824                  Hitachi Solutions America             Irvine   \n",
       "8629                          Senox Corporation             Austin   \n",
       "10260                                  TriOptus         Clarksburg   \n",
       "10333                Pinellas County Government         Clearwater   \n",
       "11414                 Resolute Consulting Group            Houston   \n",
       "13404                                  Pinecone           New York   \n",
       "13405                                  Pinecone           New York   \n",
       "13406                                  Pinecone           New York   \n",
       "13407                                  Pinecone           New York   \n",
       "14347                                   Comcast       Philadelphia   \n",
       "15390                               LTIMindtree             Mumbai   \n",
       "16659                                      Vaco          Brentwood   \n",
       "18346                        Levi Strauss & Co.      San Francisco   \n",
       "22390       Systems Planning and Analysis, Inc.         Alexandria   \n",
       "22394       Systems Planning and Analysis, Inc.         Alexandria   \n",
       "22981                                     Alcon             Geneva   \n",
       "22983                                     Alcon             Geneva   \n",
       "22985                                     Alcon             Geneva   \n",
       "22986                                     Alcon             Geneva   \n",
       "22991                                     Alcon             Geneva   \n",
       "23620                           Piper Companies             McLean   \n",
       "23625                           Piper Companies             McLean   \n",
       "23633                           Piper Companies             McLean   \n",
       "23643                           Piper Companies             McLean   \n",
       "24011                                RSM US LLP            Chicago   \n",
       "24371                       Stanford University           Stanford   \n",
       "24568                                    Prosum         El Segundo   \n",
       "24641                                      Pax8  Greenwood Village   \n",
       "24644                                      Pax8  Greenwood Village   \n",
       "25142                    Entertainment Partners            Burbank   \n",
       "28244                   Elevations Credit Union            Boulder   \n",
       "28794    Lawrence Livermore National Laboratory          Livermore   \n",
       "30410          Ohio Department of Public Safety           Columbus   \n",
       "32028  Flight Centre Travel Group, The Americas           Montvale   \n",
       "32055                          RD SOLUTIONS INC             Iselin   \n",
       "32095                                   Elevate          Charlotte   \n",
       "32096                                   Elevate          Charlotte   \n",
       "\n",
       "                State  Salary  Year  Month  Day  CS_keywords  \n",
       "1593               tx   80000   NaN    NaN  NaN            4  \n",
       "1812               PA   82366   NaN    NaN  NaN            4  \n",
       "2236               VA   85000   NaN    NaN  NaN           12  \n",
       "4589         Maryland  125000   NaN    NaN  NaN            6  \n",
       "5787       New Jersey  104000   NaN    NaN  NaN            5  \n",
       "6824               CA  175000   NaN    NaN  NaN           11  \n",
       "8629               TX  104650   NaN    NaN  NaN            6  \n",
       "10260        Maryland  108160   NaN    NaN  NaN            7  \n",
       "10333         Florida   80000   NaN    NaN  NaN            4  \n",
       "11414           Texas  200000   NaN    NaN  NaN            5  \n",
       "13404              NY  125000   NaN    NaN  NaN           19  \n",
       "13405              NY  125000   NaN    NaN  NaN           10  \n",
       "13406              NY  125000   NaN    NaN  NaN           19  \n",
       "13407              NY  125000   NaN    NaN  NaN           19  \n",
       "14347              PA   52000   NaN    NaN  NaN            9  \n",
       "15390              MH   80000   NaN    NaN  NaN            5  \n",
       "16659       Tennessee  135000   NaN    NaN  NaN            4  \n",
       "18346      California  122000   NaN    NaN  NaN           22  \n",
       "22390              VA  150000   NaN    NaN  NaN            7  \n",
       "22394              VA  130000   NaN    NaN  NaN            4  \n",
       "22981     Switzerland   93600   NaN    NaN  NaN            7  \n",
       "22983     Switzerland   93600   NaN    NaN  NaN            7  \n",
       "22985     Switzerland   93600   NaN    NaN  NaN            7  \n",
       "22986     Switzerland   93600   NaN    NaN  NaN            7  \n",
       "22991     Switzerland   93600   NaN    NaN  NaN            7  \n",
       "23620              VA  156000   NaN    NaN  NaN            5  \n",
       "23625              VA  200000   NaN    NaN  NaN            4  \n",
       "23633              VA  200000   NaN    NaN  NaN            4  \n",
       "23643              VA  125000   NaN    NaN  NaN           11  \n",
       "24011        Illinois   68640   NaN    NaN  NaN            4  \n",
       "24371              CA   79000   NaN    NaN  NaN            4  \n",
       "24568      California   51251   NaN    NaN  NaN            4  \n",
       "24641        Colorado   91000   NaN    NaN  NaN            6  \n",
       "24644        Colorado   74000   NaN    NaN  NaN            4  \n",
       "25142              CA  130000   NaN    NaN  NaN            9  \n",
       "28244        Colorado  110000   NaN    NaN  NaN            4  \n",
       "28794              CA  108840   NaN    NaN  NaN            5  \n",
       "30410            Ohio   57907   NaN    NaN  NaN            5  \n",
       "32028              NJ   75000   NaN    NaN  NaN            4  \n",
       "32055      New Jersey  135200   NaN    NaN  NaN            4  \n",
       "32095  North Carolina   52000   NaN    NaN  NaN            9  \n",
       "32096  North Carolina   41600   NaN    NaN  NaN            7  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42db156",
   "metadata": {},
   "source": [
    "## Merge the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3501d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Year: 2019.0\n",
      "Average Month: 8.991584852734922\n",
      "Average Day: 16.18162692847125\n",
      "There are no NaN values present in the 'Year' column.\n",
      "There are no NaN values present in the 'Month' column.\n",
      "There are no NaN values present in the 'Day' column.\n"
     ]
    }
   ],
   "source": [
    "big_merged_df = pd.concat([Jobspikr, Glassdoor, AI_jobs, LinkedIn], ignore_index=True, join='outer')\n",
    "\n",
    "# Calculate the average of the 'Year' column\n",
    "average_year = big_merged_df['Year'].mean()\n",
    "average_Month = big_merged_df['Month'].mean()\n",
    "average_day = big_merged_df['Day'].mean()\n",
    "\n",
    "# Print the average year\n",
    "print(\"Average Year:\", average_year)\n",
    "print(\"Average Month:\", average_Month)\n",
    "print(\"Average Day:\", average_day)\n",
    "\n",
    "# Round the average_year and convert it to an integer\n",
    "average_year = round(average_year)\n",
    "average_year = int(average_year)\n",
    "average_Month = round(average_Month)\n",
    "average_Month = int(average_Month)\n",
    "average_day = round(average_day)\n",
    "average_day = int(average_day)\n",
    "\n",
    "# Replace NaN values in the 'Year', month and day columns with the rounded integer average_year\n",
    "Glassdoor['Year'] = Glassdoor['Year'].fillna(average_year)\n",
    "Glassdoor['Month'] = Glassdoor['Month'].fillna(average_Month)\n",
    "Glassdoor['Day'] = Glassdoor['Day'].fillna(average_day)\n",
    "\n",
    "AI_jobs['Year'] = AI_jobs['Year'].fillna(average_year)\n",
    "AI_jobs['Month'] = AI_jobs['Month'].fillna(average_Month)\n",
    "AI_jobs['Day'] = AI_jobs['Day'].fillna(average_day)\n",
    "\n",
    "LinkedIn['Year'] = LinkedIn['Year'].fillna(average_year)\n",
    "LinkedIn['Month'] = LinkedIn['Month'].fillna(average_Month)\n",
    "LinkedIn['Day'] = LinkedIn['Day'].fillna(average_day)\n",
    "\n",
    "big_merged_df = pd.concat([Jobspikr, Glassdoor, AI_jobs, LinkedIn], ignore_index=True, join='outer')\n",
    "\n",
    "# For double verificastion check for NaN values in the 'Year', 'Month', and 'Day' columns\n",
    "nan_values_in_year = big_merged_df['Year'].isnull().any()\n",
    "nan_values_in_month = big_merged_df['Month'].isnull().any()\n",
    "nan_values_in_day = big_merged_df['Day'].isnull().any()\n",
    "\n",
    "if nan_values_in_year:\n",
    "    print(\"There are NaN values present in the 'Year' column.\")\n",
    "else:\n",
    "    print(\"There are no NaN values present in the 'Year' column.\")\n",
    "\n",
    "if nan_values_in_month:\n",
    "    print(\"There are NaN values present in the 'Month' column.\")\n",
    "else:\n",
    "    print(\"There are no NaN values present in the 'Month' column.\")\n",
    "\n",
    "if nan_values_in_day:\n",
    "    print(\"There are NaN values present in the 'Day' column.\")\n",
    "else:\n",
    "    print(\"There are no NaN values present in the 'Day' column.\")\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "big_merged_df.to_csv('merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "508b4e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>CS_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>90000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Carolinas HealthCare System</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1  Senior Data Scientist - Infectious Disease and...   \n",
       "2  Senior Data Scientist, Advanced Marketing Anal...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1  Senior Data Scientist - Infectious Disease and...   \n",
       "2  Senior Data Scientist, Advanced Marketing Anal...   \n",
       "\n",
       "                  Company Name       City State  Salary    Year  Month   Day  \\\n",
       "0               Morgan Stanley   New York    NY   90000  2019.0    8.0  20.0   \n",
       "1  Carolinas HealthCare System  Charlotte    NC  125000  2019.0    9.0   6.0   \n",
       "2                      Spotify   New York    NY  125000  2019.0    8.0  23.0   \n",
       "\n",
       "   CS_keywords  \n",
       "0            2  \n",
       "1            5  \n",
       "2           11  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_merged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a747d72c",
   "metadata": {},
   "source": [
    "## Push the Merged Dataframe to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df335c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'H5996Y8RGQD6ASMG',\n",
       "  'HostId': '+wjNyPdgvPDzFNsC24bKczaZezS7S/s1RAHygOpCqvgX/9rTgLjLH85VI27ycOnf1dDTEO47XOQ=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '+wjNyPdgvPDzFNsC24bKczaZezS7S/s1RAHygOpCqvgX/9rTgLjLH85VI27ycOnf1dDTEO47XOQ=',\n",
       "   'x-amz-request-id': 'H5996Y8RGQD6ASMG',\n",
       "   'date': 'Tue, 20 Feb 2024 23:34:35 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"825dbfd72a383609069c9aee459a9773\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"825dbfd72a383609069c9aee459a9773\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# Specify the new file key for the LinkedIn data\n",
    "output_file_key = 'merged_dataset.csv'\n",
    "\n",
    "# Save the LinkedIn DataFrame to a CSV file in memory\n",
    "csv_buffer = io.StringIO()\n",
    "big_merged_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload the CSV file in memory to the S3 bucket\n",
    "s3.put_object(Body=csv_buffer.getvalue(), Bucket=bucket_name, Key=output_file_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
