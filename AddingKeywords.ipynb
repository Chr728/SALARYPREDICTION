{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1014efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re \n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87444577",
   "metadata": {},
   "source": [
    "## Credentials for Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4694f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = 'AKIAZQ3DOOYC7J5PI25Z'\n",
    "aws_secret_access_key = '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'\n",
    "\n",
    "bucket_name = 'comp333bucket'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208713f0",
   "metadata": {},
   "source": [
    "## The list contains all the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b702dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_keywords = set(['python', 'pytorch', 'sql', 'mxnet', 'mlflow', 'einstein', 'theano', 'pyspark', 'solr', 'mahout',\n",
    " 'cassandra', 'aws', 'powerpoint', 'spark', 'pig', 'sas', 'java', 'nosql', 'docker', 'salesforce', 'scala', 'c++', 'net', 'tableau', 'pandas', 'scikitlearn', 'sklearn', 'matlab', 'scala', 'keras', 'tensorflow', 'clojure',\n",
    " 'caffe', 'scipy', 'numpy', 'matplotlib', 'vba', 'spss', 'linux', 'azure', 'cloud', 'gcp', 'mongodb', 'mysql', 'oracle',\n",
    " 'redshift', 'snowflake', 'kafka', 'javascript', 'qlik', 'jupyter', 'perl', 'bigquery', 'unix', 'react',\n",
    " 'scikit', 'powerbi', 's3', 'ec2', 'lambda', 'ssrs', 'kubernetes', 'hana', 'spacy', 'tf', 'django', 'sagemaker',\n",
    " 'seaborn', 'mllib', 'github', 'git', 'elasticsearch', 'splunk', 'airflow', 'looker', 'rapidminer', 'birt', 'pentaho',\n",
    "'jquery', 'nodejs', 'd3', 'plotly', 'bokeh', 'xgboost', 'rstudio', 'shiny', 'dash', 'h20', 'h2o', 'hadoop', 'mapreduce',\n",
    " 'hive', 'cognos', 'angular', 'nltk', 'flask', 'node', 'firebase', 'bigtable', 'rust', 'php', 'cntk', 'lightgbm',\n",
    " 'kubeflow', 'rpython', 'unixlinux', 'postgressql', 'postgresql', 'postgres', 'hbase', 'dask', 'ruby', 'julia', 'tensor',\n",
    " 'dplyr','ggplot2','esquisse','bioconductor','shiny','lubridate','knitr','mlr','quanteda','dt','rcrawler','caret','rmarkdown',\n",
    " 'leaflet','janitor','ggvis','plotly','rcharts','rbokeh','broom','stringr','magrittr','slidify','rvest',\n",
    " 'rmysql','rsqlite','prophet','glmnet','text2vec','snowballc','quantmod','rstan','swirl','datasciencer', \n",
    " 'amazon web services', 'google cloud', 'sql server',\n",
    "  'cleansing', 'chatbot', 'cleaning', 'blockchain', 'causality', 'correlation', 'bandit', 'anomaly', 'kpi',\n",
    " 'dashboard', 'geospatial', 'ocr',  'pca', 'gis', 'svm', 'svd', 'tuning', 'hyperparameter', 'hypothesis',\n",
    " 'salesforcecom', 'segmentation', 'biostatistics', 'unsupervised', 'supervised', 'exploratory',\n",
    " 'recommender', 'recommendations', 'research', 'sequencing', 'probability', 'reinforcement', 'graph', 'bioinformatics',\n",
    "  'knn', 'etl', 'normalization', 'classification', 'optimizing', 'prediction', 'forecasting',\n",
    " 'clustering', 'optimization', 'visualization', 'nlp', 'c#',\n",
    " 'regression', 'logistic', 'cnn', 'glm',\n",
    " 'rnn', 'lstm', 'gbm', 'boosting', 'recurrent', 'convolutional', 'bayesian',\n",
    " 'bayes', 'random forest', 'natural language processing', 'machine learning', 'decision tree', 'deep learning', 'experimental design',\n",
    " 'time series', 'nearest neighbors', 'neural network', 'support vector machine', 'computer vision', 'machine vision', 'dimensionality reduction',\n",
    " 'text analytics',  'a/b testing', 'data mining', 'kajsadouas', 'senior','intern'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e0bb4",
   "metadata": {},
   "source": [
    "## Get the list of keywords that actualling appear in the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a5067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'merged_dataset.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = obj['Body'].read()\n",
    "merged_df = pd.read_csv(BytesIO(content), engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c446662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['segmentation', 'node', 'sas', 'redshift', 'random forest', 'classification', 'django', 'logistic', 'salesforce', 'bandit', 'optimization', 'snowflake', 'text analytics', 'tf', 'scipy', 'powerbi', 'tensorflow', 'boosting', 'flask', 'numpy', 'net', 'spacy', 'biostatistics', 'anomaly', 'data mining', 'pyspark', 'pytorch', 'geospatial', 'supervised', 'tensor', 'dimensionality reduction', 'pca', 'chatbot', 'a/b testing', 'natural language processing', 'qlik', 'lambda', 'rvest', 'blockchain', 'decision tree', 'intern', 'ocr', 'research', 'bioinformatics', 'recurrent', 'sql', 'recommendations', 'causality', 'support vector machine', 'machine learning', 'prediction', 'xgboost', 'docker', 'senior', 'keras', 'probability', 'tableau', 'dash', 'rstan', 'java', 'sql server', 'python', 'sequencing', 'h2o', 'hbase', 'nearest neighbors', 'neural network', 'graph', 'experimental design', 'recommender', 'computer vision', 'machine vision', 'plotly', 'forecasting', 'time series', 'kubernetes', 'spark', 'php', 'pandas', 'bokeh', 'd3', 'correlation', 'exploratory', 'scala', 'theano', 's3', 'github', 'cleaning', 'ggplot2', 'optimizing', 'birt', 'dt', 'hypothesis', 'unsupervised', 'git', 'normalization', 'jupyter', 'seaborn', 'ssrs', 'perl', 'angular', 'matplotlib', 'dashboard', 'scikit', 'gis', 'clustering', 'aws', 'cleansing', 'rust', 'javascript', 'shiny', 'azure', 'hive', 'regression', 'tuning', 'nltk', 'hyperparameter', 'linux', 'react', 'deep learning', 'etl', 'elasticsearch', 'convolutional', 'cloud', 'reinforcement', 'mysql', 'visualization', 'sklearn', 'unix']\n"
     ]
    }
   ],
   "source": [
    "CS_word_counts = {word: merged_df['Description'].str.count(re.escape(word)).sum() for word in CS_keywords}\n",
    "CS_keyword_freq = pd.DataFrame(list(CS_word_counts.items()), columns=['CS_keyword', 'CS_keyword_Count'])\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "CS_keyword_freq = CS_keyword_freq[CS_keyword_freq['CS_keyword_Count'] !=0]\n",
    "\n",
    "print(CS_keyword_freq['CS_keyword'].tolist())\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb143b9f",
   "metadata": {},
   "source": [
    "## Adding the keywords columns in the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4289c8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_3465/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>CS_keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>deep learning</th>\n",
       "      <th>etl</th>\n",
       "      <th>elasticsearch</th>\n",
       "      <th>convolutional</th>\n",
       "      <th>cloud</th>\n",
       "      <th>reinforcement</th>\n",
       "      <th>mysql</th>\n",
       "      <th>visualization</th>\n",
       "      <th>sklearn</th>\n",
       "      <th>unix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>90000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Carolinas HealthCare System</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TV Data Scientist in Burlington, MA</td>\n",
       "      <td>TV Data Scientist in Burlington, MA Our servic...</td>\n",
       "      <td>LiveRamp</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>MA</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TV Data Scientist in Burlington, MA</td>\n",
       "      <td>TV Data Scientist in Burlington, MA ABOUT THIS...</td>\n",
       "      <td>LiveRamp</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>MA</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>Epidemiology Investigator 2 (Hybrid Eligible)</td>\n",
       "      <td>About Us Our mission is to save lives, reduce ...</td>\n",
       "      <td>Ohio Department of Public Safety</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>57907</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Wholesale Business Analyst - Montvale, NJ (Hyb...</td>\n",
       "      <td>Wholesale Business Analyst - Montvale, NJ (Hyb...</td>\n",
       "      <td>Flight Centre Travel Group, The Americas</td>\n",
       "      <td>Montvale</td>\n",
       "      <td>NJ</td>\n",
       "      <td>75000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>AEM Developer</td>\n",
       "      <td>Senior AEM Developer | Mountain view, CA | Pho...</td>\n",
       "      <td>RD SOLUTIONS INC</td>\n",
       "      <td>Iselin</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>135200</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>MBA Fall Intern, Insights</td>\n",
       "      <td>ABOUT ELEVATE SPORTS VENTURES:Elevate Sports V...</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>52000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Fall Undergraduate Intern, Insights</td>\n",
       "      <td>ABOUT ELEVATE SPORTS VENTURES: Elevate Sports ...</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>41600</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1468 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "0     Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1     Senior Data Scientist - Infectious Disease and...   \n",
       "2     Senior Data Scientist, Advanced Marketing Anal...   \n",
       "3                   TV Data Scientist in Burlington, MA   \n",
       "4                   TV Data Scientist in Burlington, MA   \n",
       "...                                                 ...   \n",
       "1463      Epidemiology Investigator 2 (Hybrid Eligible)   \n",
       "1464  Wholesale Business Analyst - Montvale, NJ (Hyb...   \n",
       "1465                                      AEM Developer   \n",
       "1466                          MBA Fall Intern, Insights   \n",
       "1467                Fall Undergraduate Intern, Insights   \n",
       "\n",
       "                                            Description  \\\n",
       "0     Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1     Senior Data Scientist - Infectious Disease and...   \n",
       "2     Senior Data Scientist, Advanced Marketing Anal...   \n",
       "3     TV Data Scientist in Burlington, MA Our servic...   \n",
       "4     TV Data Scientist in Burlington, MA ABOUT THIS...   \n",
       "...                                                 ...   \n",
       "1463  About Us Our mission is to save lives, reduce ...   \n",
       "1464  Wholesale Business Analyst - Montvale, NJ (Hyb...   \n",
       "1465  Senior AEM Developer | Mountain view, CA | Pho...   \n",
       "1466  ABOUT ELEVATE SPORTS VENTURES:Elevate Sports V...   \n",
       "1467  ABOUT ELEVATE SPORTS VENTURES: Elevate Sports ...   \n",
       "\n",
       "                                  Company Name        City           State  \\\n",
       "0                               Morgan Stanley    New York              NY   \n",
       "1                  Carolinas HealthCare System   Charlotte              NC   \n",
       "2                                      Spotify    New York              NY   \n",
       "3                                     LiveRamp  Burlington              MA   \n",
       "4                                     LiveRamp  Burlington              MA   \n",
       "...                                        ...         ...             ...   \n",
       "1463          Ohio Department of Public Safety    Columbus            Ohio   \n",
       "1464  Flight Centre Travel Group, The Americas    Montvale              NJ   \n",
       "1465                          RD SOLUTIONS INC      Iselin      New Jersey   \n",
       "1466                                   Elevate   Charlotte  North Carolina   \n",
       "1467                                   Elevate   Charlotte  North Carolina   \n",
       "\n",
       "      Salary    Year  Month   Day  CS_keywords  ...  deep learning  etl  \\\n",
       "0      90000  2019.0    8.0  20.0            2  ...              0    0   \n",
       "1     125000  2019.0    9.0   6.0            5  ...              0    0   \n",
       "2     125000  2019.0    8.0  23.0           11  ...              0    0   \n",
       "3     125000  2019.0    8.0   2.0           13  ...              0    0   \n",
       "4     125000  2019.0    8.0   2.0           13  ...              0    0   \n",
       "...      ...     ...    ...   ...          ...  ...            ...  ...   \n",
       "1463   57907  2019.0    9.0  16.0            5  ...              0    0   \n",
       "1464   75000  2019.0    9.0  16.0            4  ...              0    0   \n",
       "1465  135200  2019.0    9.0  16.0            4  ...              0    0   \n",
       "1466   52000  2019.0    9.0  16.0            9  ...              0    0   \n",
       "1467   41600  2019.0    9.0  16.0            7  ...              0    0   \n",
       "\n",
       "      elasticsearch  convolutional  cloud  reinforcement  mysql  \\\n",
       "0                 0              0      0              0      0   \n",
       "1                 0              0      0              0      0   \n",
       "2                 0              0      0              0      0   \n",
       "3                 0              0      0              0      0   \n",
       "4                 0              0      0              0      0   \n",
       "...             ...            ...    ...            ...    ...   \n",
       "1463              0              0      0              0      0   \n",
       "1464              0              0      0              0      0   \n",
       "1465              0              0      0              0      0   \n",
       "1466              0              0      0              0      0   \n",
       "1467              0              0      0              0      0   \n",
       "\n",
       "      visualization  sklearn  unix  \n",
       "0                 1        0     0  \n",
       "1                 1        0     0  \n",
       "2                 1        0     0  \n",
       "3                 1        0     0  \n",
       "4                 1        0     0  \n",
       "...             ...      ...   ...  \n",
       "1463              0        0     0  \n",
       "1464              0        0     0  \n",
       "1465              0        0     0  \n",
       "1466              4        0     0  \n",
       "1467              3        0     0  \n",
       "\n",
       "[1468 rows x 139 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for keyword in CS_keyword_freq['CS_keyword'].tolist():\n",
    "    merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867ce41",
   "metadata": {},
   "source": [
    "## Chekcing correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e41861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary                         1.000000\n",
      "python                         0.196211\n",
      "CS_keywords                    0.182668\n",
      "rstan                          0.133911\n",
      "sql                            0.116172\n",
      "intern                         0.114538\n",
      "tableau                        0.084746\n",
      "optimization                   0.083686\n",
      "dashboard                      0.080809\n",
      "visualization                  0.079992\n",
      "dash                           0.079880\n",
      "recommendations                0.079378\n",
      "classification                 0.078076\n",
      "birt                           0.073027\n",
      "exploratory                    0.072822\n",
      "scala                          0.070857\n",
      "spark                          0.070423\n",
      "net                            0.069182\n",
      "deep learning                  0.068271\n",
      "gis                            0.064912\n",
      "clustering                     0.064539\n",
      "regression                     0.063979\n",
      "redshift                       0.063977\n",
      "machine learning               0.063627\n",
      "git                            0.061985\n",
      "neural network                 0.061120\n",
      "jupyter                        0.061079\n",
      "cleaning                       0.059791\n",
      "a/b testing                    0.059387\n",
      "hive                           0.057402\n",
      "data mining                    0.056821\n",
      "tf                             0.055699\n",
      "research                       0.055101\n",
      "docker                         0.054552\n",
      "unsupervised                   0.053951\n",
      "random forest                  0.053211\n",
      "supervised                     0.052698\n",
      "mysql                          0.052040\n",
      "sas                            0.051956\n",
      "logistic                       0.051801\n",
      "hypothesis                     0.051302\n",
      "php                            0.050920\n",
      "decision tree                  0.050088\n",
      "aws                            0.049929\n",
      "tensor                         0.049612\n",
      "pca                            0.049552\n",
      "dimensionality reduction       0.049426\n",
      "scikit                         0.047825\n",
      "kubernetes                     0.047081\n",
      "perl                           0.046839\n",
      "s3                             0.046672\n",
      "tensorflow                     0.046459\n",
      "sql server                     0.046449\n",
      "pandas                         0.044925\n",
      "anomaly                        0.042648\n",
      "natural language processing    0.041300\n",
      "senior                         0.040473\n",
      "pytorch                        0.039395\n",
      "etl                            0.038794\n",
      "correlation                    0.038485\n",
      "pyspark                        0.038088\n",
      "sklearn                        0.038014\n",
      "java                           0.037314\n",
      "shiny                          0.036657\n",
      "dt                             0.035754\n",
      "h2o                            0.035566\n",
      "javascript                     0.034622\n",
      "node                           0.034411\n",
      "scipy                          0.033817\n",
      "numpy                          0.033645\n",
      "keras                          0.032452\n",
      "biostatistics                  0.031406\n",
      "forecasting                    0.031307\n",
      "rvest                          0.031090\n",
      "qlik                           0.030806\n",
      "bokeh                          0.030082\n",
      "ggplot2                        0.029653\n",
      "linux                          0.029562\n",
      "machine vision                 0.028058\n",
      "azure                          0.027611\n",
      "geospatial                     0.027299\n",
      "github                         0.027137\n",
      "hyperparameter                 0.026832\n",
      "lambda                         0.026676\n",
      "snowflake                      0.026509\n",
      "salesforce                     0.026314\n",
      "experimental design            0.026037\n",
      "bioinformatics                 0.026020\n",
      "ssrs                           0.025302\n",
      "cleansing                      0.024313\n",
      "matplotlib                     0.023530\n",
      "blockchain                     0.023131\n",
      "recurrent                      0.022595\n",
      "Month                          0.022445\n",
      "Day                            0.020837\n",
      "recommender                    0.019960\n",
      "cloud                          0.019947\n",
      "nearest neighbors              0.018967\n",
      "computer vision                0.017639\n",
      "causality                      0.015315\n",
      "chatbot                        0.014826\n",
      "powerbi                        0.014284\n",
      "plotly                         0.014249\n",
      "support vector machine         0.013576\n",
      "rust                           0.013373\n",
      "xgboost                        0.011306\n",
      "text analytics                 0.011302\n",
      "flask                          0.011099\n",
      "django                         0.011001\n",
      "reinforcement                  0.009543\n",
      "elasticsearch                  0.009258\n",
      "seaborn                        0.009038\n",
      "react                          0.008196\n",
      "probability                    0.007315\n",
      "tuning                         0.007290\n",
      "theano                         0.007116\n",
      "boosting                       0.007010\n",
      "hbase                          0.006579\n",
      "angular                        0.006547\n",
      "graph                          0.005968\n",
      "time series                    0.005868\n",
      "convolutional                  0.005834\n",
      "prediction                     0.004863\n",
      "nltk                           0.004538\n",
      "spacy                          0.004313\n",
      "d3                             0.004035\n",
      "unix                           0.003137\n",
      "segmentation                   0.002742\n",
      "ocr                            0.002715\n",
      "sequencing                     0.002631\n",
      "optimizing                     0.000957\n",
      "bandit                         0.000690\n",
      "normalization                  0.000472\n",
      "Year                                NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "merged_df_corr = merged_df.drop(['Description', 'Title', 'Company Name', 'City', 'State'], axis=1)\n",
    "\n",
    "salary_column = merged_df_corr['Salary']\n",
    "correlations = merged_df_corr.corrwith(salary_column)\n",
    "\n",
    "\n",
    "sorted_correlations = correlations.abs().sort_values(ascending=False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(sorted_correlations)\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b2576",
   "metadata": {},
   "source": [
    "## Select the top 20 correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe95862",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Date'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCompany Name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mState\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSalary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCS_keywords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrstan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mintern\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimization\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtableau\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdashboard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecommendations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvisualization\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscala\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexploratory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeep learning\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbirt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m final_df\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/comp472/lib/python3.8/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/comp472/lib/python3.8/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/comp472/lib/python3.8/site-packages/pandas/core/indexes/base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Date'] not in index\""
     ]
    }
   ],
   "source": [
    "final_df = merged_df[['Description', 'Title', 'Company Name', 'City', 'Year', 'Date', 'Month','State','Salary'\n",
    "                      ,'python','CS_keywords','rstan' ,'sql','intern', 'optimization','spark'\n",
    "                      ,'tableau','dashboard','classification','recommendations','dash','visualization'\n",
    "                      ,'scala','exploratory','deep learning','birt', 'net']]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8de66a",
   "metadata": {},
   "source": [
    "## Push the dataset to Amazon S3 datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb242bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1NE22QWDRTAT4WF9',\n",
       "  'HostId': 'dGwm8YdAiLXGNYHpsb1AH9CyGj/mSoaGAse7wfxaOKc0PGd0pJLO62s/hC8SoyheOw3LfZeH2MA=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'dGwm8YdAiLXGNYHpsb1AH9CyGj/mSoaGAse7wfxaOKc0PGd0pJLO62s/hC8SoyheOw3LfZeH2MA=',\n",
       "   'x-amz-request-id': '1NE22QWDRTAT4WF9',\n",
       "   'date': 'Tue, 20 Feb 2024 22:11:13 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"ece46290539378ff1a98beac84ac9fbd\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"ece46290539378ff1a98beac84ac9fbd\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file_key = 'with_20_keywords.csv'\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "final_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3.put_object(Body = csv_buffer.getvalue(), Bucket = bucket_name, Key=output_file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef0886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
