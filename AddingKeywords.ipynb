{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1014efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re \n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import io\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87444577",
   "metadata": {},
   "source": [
    "## Credentials for Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4694f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = 'AKIAZQ3DOOYC7J5PI25Z'\n",
    "aws_secret_access_key = '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'\n",
    "\n",
    "bucket_name = 'comp333bucket'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208713f0",
   "metadata": {},
   "source": [
    "## The list contains all the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b702dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_keywords = set(['python', 'pytorch', 'sql', 'mxnet', 'mlflow', 'einstein', 'theano', 'pyspark', 'solr', 'mahout',\n",
    " 'cassandra', 'aws', 'powerpoint', 'spark', 'pig', 'sas', 'java', 'nosql', 'docker', 'salesforce', 'scala', 'c++', 'net', 'tableau', 'pandas', 'scikitlearn', 'sklearn', 'matlab', 'scala', 'keras', 'tensorflow', 'clojure',\n",
    " 'caffe', 'scipy', 'numpy', 'matplotlib', 'vba', 'spss', 'linux', 'azure', 'cloud', 'gcp', 'mongodb', 'mysql', 'oracle',\n",
    " 'redshift', 'snowflake', 'kafka', 'javascript', 'qlik', 'jupyter', 'perl', 'bigquery', 'unix', 'react',\n",
    " 'scikit', 'powerbi', 's3', 'ec2', 'lambda', 'ssrs', 'kubernetes', 'hana', 'spacy', 'tf', 'django', 'sagemaker',\n",
    " 'seaborn', 'mllib', 'github', 'git', 'elasticsearch', 'splunk', 'airflow', 'looker', 'rapidminer', 'birt', 'pentaho',\n",
    "'jquery', 'nodejs', 'd3', 'plotly', 'bokeh', 'xgboost', 'rstudio', 'shiny', 'dash', 'h20', 'h2o', 'hadoop', 'mapreduce',\n",
    " 'hive', 'cognos', 'angular', 'nltk', 'flask', 'node', 'firebase', 'bigtable', 'rust', 'php', 'cntk', 'lightgbm',\n",
    " 'kubeflow', 'rpython', 'unixlinux', 'postgressql', 'postgresql', 'postgres', 'hbase', 'dask', 'ruby', 'julia', 'tensor',\n",
    " 'dplyr','ggplot2','esquisse','bioconductor','shiny','lubridate','knitr','mlr','quanteda','dt','rcrawler','caret','rmarkdown',\n",
    " 'leaflet','janitor','ggvis','plotly','rcharts','rbokeh','broom','stringr','magrittr','slidify','rvest',\n",
    " 'rmysql','rsqlite','prophet','glmnet','text2vec','snowballc','quantmod','rstan','swirl','datasciencer', \n",
    " 'amazon web services', 'google cloud', 'sql server',\n",
    "  'cleansing', 'chatbot', 'cleaning', 'blockchain', 'causality', 'correlation', 'bandit', 'anomaly', 'kpi',\n",
    " 'dashboard', 'geospatial', 'ocr',  'pca', 'gis', 'svm', 'svd', 'tuning', 'hyperparameter', 'hypothesis',\n",
    " 'salesforcecom', 'segmentation', 'biostatistics', 'unsupervised', 'supervised', 'exploratory',\n",
    " 'recommender', 'recommendations', 'research', 'sequencing', 'probability', 'reinforcement', 'graph', 'bioinformatics',\n",
    "  'knn', 'etl', 'normalization', 'classification', 'optimizing', 'prediction', 'forecasting',\n",
    " 'clustering', 'optimization', 'visualization', 'nlp', 'c#',\n",
    " 'regression', 'logistic', 'cnn', 'glm',\n",
    " 'rnn', 'lstm', 'gbm', 'boosting', 'recurrent', 'convolutional', 'bayesian',\n",
    " 'bayes', 'random forest', 'natural language processing', 'machine learning', 'decision tree', 'deep learning', 'experimental design',\n",
    " 'time series', 'nearest neighbors', 'neural network', 'support vector machine', 'computer vision', 'machine vision', 'dimensionality reduction',\n",
    " 'text analytics',  'a/b testing', 'data mining', 'kajsadouas', 'senior','intern'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e0bb4",
   "metadata": {},
   "source": [
    "## Get the list of keywords that actualling appear in the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a5067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'merged_dataset.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = obj['Body'].read()\n",
    "merged_df = pd.read_csv(BytesIO(content), engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7cad102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>CS_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>90000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Carolinas HealthCare System</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TV Data Scientist in Burlington, MA</td>\n",
       "      <td>TV Data Scientist in Burlington, MA Our servic...</td>\n",
       "      <td>LiveRamp</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>MA</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TV Data Scientist in Burlington, MA</td>\n",
       "      <td>TV Data Scientist in Burlington, MA ABOUT THIS...</td>\n",
       "      <td>LiveRamp</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>MA</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>Epidemiology Investigator 2 (Hybrid Eligible)</td>\n",
       "      <td>About Us Our mission is to save lives, reduce ...</td>\n",
       "      <td>Ohio Department of Public Safety</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>57907</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Wholesale Business Analyst - Montvale, NJ (Hyb...</td>\n",
       "      <td>Wholesale Business Analyst - Montvale, NJ (Hyb...</td>\n",
       "      <td>Flight Centre Travel Group, The Americas</td>\n",
       "      <td>Montvale</td>\n",
       "      <td>NJ</td>\n",
       "      <td>75000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>AEM Developer</td>\n",
       "      <td>Senior AEM Developer | Mountain view, CA | Pho...</td>\n",
       "      <td>RD SOLUTIONS INC</td>\n",
       "      <td>Iselin</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>135200</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>MBA Fall Intern, Insights</td>\n",
       "      <td>ABOUT ELEVATE SPORTS VENTURES:Elevate Sports V...</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>52000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Fall Undergraduate Intern, Insights</td>\n",
       "      <td>ABOUT ELEVATE SPORTS VENTURES: Elevate Sports ...</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>41600</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1468 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "0     Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1     Senior Data Scientist - Infectious Disease and...   \n",
       "2     Senior Data Scientist, Advanced Marketing Anal...   \n",
       "3                   TV Data Scientist in Burlington, MA   \n",
       "4                   TV Data Scientist in Burlington, MA   \n",
       "...                                                 ...   \n",
       "1463      Epidemiology Investigator 2 (Hybrid Eligible)   \n",
       "1464  Wholesale Business Analyst - Montvale, NJ (Hyb...   \n",
       "1465                                      AEM Developer   \n",
       "1466                          MBA Fall Intern, Insights   \n",
       "1467                Fall Undergraduate Intern, Insights   \n",
       "\n",
       "                                            Description  \\\n",
       "0     Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1     Senior Data Scientist - Infectious Disease and...   \n",
       "2     Senior Data Scientist, Advanced Marketing Anal...   \n",
       "3     TV Data Scientist in Burlington, MA Our servic...   \n",
       "4     TV Data Scientist in Burlington, MA ABOUT THIS...   \n",
       "...                                                 ...   \n",
       "1463  About Us Our mission is to save lives, reduce ...   \n",
       "1464  Wholesale Business Analyst - Montvale, NJ (Hyb...   \n",
       "1465  Senior AEM Developer | Mountain view, CA | Pho...   \n",
       "1466  ABOUT ELEVATE SPORTS VENTURES:Elevate Sports V...   \n",
       "1467  ABOUT ELEVATE SPORTS VENTURES: Elevate Sports ...   \n",
       "\n",
       "                                  Company Name        City           State  \\\n",
       "0                               Morgan Stanley    New York              NY   \n",
       "1                  Carolinas HealthCare System   Charlotte              NC   \n",
       "2                                      Spotify    New York              NY   \n",
       "3                                     LiveRamp  Burlington              MA   \n",
       "4                                     LiveRamp  Burlington              MA   \n",
       "...                                        ...         ...             ...   \n",
       "1463          Ohio Department of Public Safety    Columbus            Ohio   \n",
       "1464  Flight Centre Travel Group, The Americas    Montvale              NJ   \n",
       "1465                          RD SOLUTIONS INC      Iselin      New Jersey   \n",
       "1466                                   Elevate   Charlotte  North Carolina   \n",
       "1467                                   Elevate   Charlotte  North Carolina   \n",
       "\n",
       "      Salary    Year  Month   Day  CS_keywords  \n",
       "0      90000  2019.0    8.0  20.0            2  \n",
       "1     125000  2019.0    9.0   6.0            5  \n",
       "2     125000  2019.0    8.0  23.0           11  \n",
       "3     125000  2019.0    8.0   2.0           13  \n",
       "4     125000  2019.0    8.0   2.0           13  \n",
       "...      ...     ...    ...   ...          ...  \n",
       "1463   57907  2019.0    9.0  16.0            5  \n",
       "1464   75000  2019.0    9.0  16.0            4  \n",
       "1465  135200  2019.0    9.0  16.0            4  \n",
       "1466   52000  2019.0    9.0  16.0            9  \n",
       "1467   41600  2019.0    9.0  16.0            7  \n",
       "\n",
       "[1468 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c446662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spacy', 'computer vision', 'clustering', 'a/b testing', 'qlik', 'causality', 'supervised', 'hypothesis', 'redshift', 'ggplot2', 'elasticsearch', 'sas', 'net', 'sklearn', 'dt', 'lambda', 'sequencing', 'mysql', 'tensorflow', 'jupyter', 'ssrs', 'research', 'tableau', 'python', 'birt', 'shiny', 'prediction', 'geospatial', 'git', 'java', 'react', 'bokeh', 'tf', 'scikit', 'tensor', 'recommender', 'machine vision', 'spark', 'aws', 's3', 'senior', 'pandas', 'data mining', 'keras', 'reinforcement', 'boosting', 'd3', 'chatbot', 'dimensionality reduction', 'salesforce', 'hbase', 'hyperparameter', 'matplotlib', 'javascript', 'forecasting', 'support vector machine', 'regression', 'ocr', 'decision tree', 'logistic', 'etl', 'nltk', 'cloud', 'linux', 'pytorch', 'time series', 'intern', 'php', 'exploratory', 'visualization', 'natural language processing', 'anomaly', 'optimization', 'classification', 'neural network', 'bioinformatics', 'blockchain', 'docker', 'cleansing', 'pca', 'azure', 'scala', 'sql server', 'angular', 'theano', 'normalization', 'hive', 'optimizing', 'h2o', 'segmentation', 'convolutional', 'xgboost', 'dashboard', 'rust', 'powerbi', 'node', 'unix', 'probability', 'snowflake', 'pyspark', 'dash', 'machine learning', 'recurrent', 'deep learning', 'recommendations', 'biostatistics', 'gis', 'github', 'cleaning', 'django', 'flask', 'plotly', 'correlation', 'random forest', 'rvest', 'graph', 'nearest neighbors', 'seaborn', 'bandit', 'kubernetes', 'rstan', 'unsupervised', 'perl', 'tuning', 'text analytics', 'numpy', 'experimental design', 'scipy', 'sql']\n"
     ]
    }
   ],
   "source": [
    "CS_word_counts = {word: merged_df['Description'].str.count(re.escape(word)).sum() for word in CS_keywords}\n",
    "CS_keyword_freq = pd.DataFrame(list(CS_word_counts.items()), columns=['CS_keyword', 'CS_keyword_Count'])\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "CS_keyword_freq = CS_keyword_freq[CS_keyword_freq['CS_keyword_Count'] !=0]\n",
    "\n",
    "print(CS_keyword_freq['CS_keyword'].tolist())\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb143b9f",
   "metadata": {},
   "source": [
    "## Adding the keywords columns in the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4289c8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
      "/var/folders/0l/srjf8wrn45d3c527z89b0d9m0000gn/T/ipykernel_7982/263324470.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>CS_keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>kubernetes</th>\n",
       "      <th>rstan</th>\n",
       "      <th>unsupervised</th>\n",
       "      <th>perl</th>\n",
       "      <th>tuning</th>\n",
       "      <th>text analytics</th>\n",
       "      <th>numpy</th>\n",
       "      <th>experimental design</th>\n",
       "      <th>scipy</th>\n",
       "      <th>sql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>90000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Carolinas HealthCare System</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>NC</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TV Data Scientist in Burlington, MA</td>\n",
       "      <td>TV Data Scientist in Burlington, MA Our servic...</td>\n",
       "      <td>LiveRamp</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>MA</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TV Data Scientist in Burlington, MA</td>\n",
       "      <td>TV Data Scientist in Burlington, MA ABOUT THIS...</td>\n",
       "      <td>LiveRamp</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>MA</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>Epidemiology Investigator 2 (Hybrid Eligible)</td>\n",
       "      <td>About Us Our mission is to save lives, reduce ...</td>\n",
       "      <td>Ohio Department of Public Safety</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>57907</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Wholesale Business Analyst - Montvale, NJ (Hyb...</td>\n",
       "      <td>Wholesale Business Analyst - Montvale, NJ (Hyb...</td>\n",
       "      <td>Flight Centre Travel Group, The Americas</td>\n",
       "      <td>Montvale</td>\n",
       "      <td>NJ</td>\n",
       "      <td>75000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>AEM Developer</td>\n",
       "      <td>Senior AEM Developer | Mountain view, CA | Pho...</td>\n",
       "      <td>RD SOLUTIONS INC</td>\n",
       "      <td>Iselin</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>135200</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>MBA Fall Intern, Insights</td>\n",
       "      <td>ABOUT ELEVATE SPORTS VENTURES:Elevate Sports V...</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>52000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Fall Undergraduate Intern, Insights</td>\n",
       "      <td>ABOUT ELEVATE SPORTS VENTURES: Elevate Sports ...</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>41600</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1468 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "0     Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1     Senior Data Scientist - Infectious Disease and...   \n",
       "2     Senior Data Scientist, Advanced Marketing Anal...   \n",
       "3                   TV Data Scientist in Burlington, MA   \n",
       "4                   TV Data Scientist in Burlington, MA   \n",
       "...                                                 ...   \n",
       "1463      Epidemiology Investigator 2 (Hybrid Eligible)   \n",
       "1464  Wholesale Business Analyst - Montvale, NJ (Hyb...   \n",
       "1465                                      AEM Developer   \n",
       "1466                          MBA Fall Intern, Insights   \n",
       "1467                Fall Undergraduate Intern, Insights   \n",
       "\n",
       "                                            Description  \\\n",
       "0     Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1     Senior Data Scientist - Infectious Disease and...   \n",
       "2     Senior Data Scientist, Advanced Marketing Anal...   \n",
       "3     TV Data Scientist in Burlington, MA Our servic...   \n",
       "4     TV Data Scientist in Burlington, MA ABOUT THIS...   \n",
       "...                                                 ...   \n",
       "1463  About Us Our mission is to save lives, reduce ...   \n",
       "1464  Wholesale Business Analyst - Montvale, NJ (Hyb...   \n",
       "1465  Senior AEM Developer | Mountain view, CA | Pho...   \n",
       "1466  ABOUT ELEVATE SPORTS VENTURES:Elevate Sports V...   \n",
       "1467  ABOUT ELEVATE SPORTS VENTURES: Elevate Sports ...   \n",
       "\n",
       "                                  Company Name        City           State  \\\n",
       "0                               Morgan Stanley    New York              NY   \n",
       "1                  Carolinas HealthCare System   Charlotte              NC   \n",
       "2                                      Spotify    New York              NY   \n",
       "3                                     LiveRamp  Burlington              MA   \n",
       "4                                     LiveRamp  Burlington              MA   \n",
       "...                                        ...         ...             ...   \n",
       "1463          Ohio Department of Public Safety    Columbus            Ohio   \n",
       "1464  Flight Centre Travel Group, The Americas    Montvale              NJ   \n",
       "1465                          RD SOLUTIONS INC      Iselin      New Jersey   \n",
       "1466                                   Elevate   Charlotte  North Carolina   \n",
       "1467                                   Elevate   Charlotte  North Carolina   \n",
       "\n",
       "      Salary    Year  Month   Day  CS_keywords  ...  kubernetes  rstan  \\\n",
       "0      90000  2019.0    8.0  20.0            2  ...           0      0   \n",
       "1     125000  2019.0    9.0   6.0            5  ...           0      1   \n",
       "2     125000  2019.0    8.0  23.0           11  ...           0      1   \n",
       "3     125000  2019.0    8.0   2.0           13  ...           0      2   \n",
       "4     125000  2019.0    8.0   2.0           13  ...           0      2   \n",
       "...      ...     ...    ...   ...          ...  ...         ...    ...   \n",
       "1463   57907  2019.0    9.0  16.0            5  ...           0      0   \n",
       "1464   75000  2019.0    9.0  16.0            4  ...           0      5   \n",
       "1465  135200  2019.0    9.0  16.0            4  ...           0      1   \n",
       "1466   52000  2019.0    9.0  16.0            9  ...           0      0   \n",
       "1467   41600  2019.0    9.0  16.0            7  ...           0      0   \n",
       "\n",
       "      unsupervised  perl  tuning  text analytics  numpy  experimental design  \\\n",
       "0                0     0       0               0      0                    0   \n",
       "1                0     0       0               0      0                    0   \n",
       "2                0     0       0               0      0                    1   \n",
       "3                0     1       0               0      0                    0   \n",
       "4                0     1       0               0      0                    0   \n",
       "...            ...   ...     ...             ...    ...                  ...   \n",
       "1463             0     0       0               0      0                    0   \n",
       "1464             0     0       0               0      0                    0   \n",
       "1465             0     0       0               0      0                    0   \n",
       "1466             0     0       0               0      0                    0   \n",
       "1467             0     0       0               0      0                    0   \n",
       "\n",
       "      scipy  sql  \n",
       "0         0    0  \n",
       "1         0    0  \n",
       "2         0    1  \n",
       "3         0    1  \n",
       "4         0    1  \n",
       "...     ...  ...  \n",
       "1463      0    0  \n",
       "1464      0    2  \n",
       "1465      0    0  \n",
       "1466      0    1  \n",
       "1467      0    1  \n",
       "\n",
       "[1468 rows x 139 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for keyword in CS_keyword_freq['CS_keyword'].tolist():\n",
    "    merged_df[keyword] = merged_df['Description'].apply(lambda x: x.lower().count(keyword.lower()))\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867ce41",
   "metadata": {},
   "source": [
    "## Chekcing correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e41861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary                         1.000000\n",
      "python                         0.196211\n",
      "CS_keywords                    0.182668\n",
      "rstan                          0.133911\n",
      "sql                            0.116172\n",
      "intern                         0.114538\n",
      "tableau                        0.084746\n",
      "optimization                   0.083686\n",
      "dashboard                      0.080809\n",
      "visualization                  0.079992\n",
      "dash                           0.079880\n",
      "recommendations                0.079378\n",
      "classification                 0.078076\n",
      "birt                           0.073027\n",
      "exploratory                    0.072822\n",
      "scala                          0.070857\n",
      "spark                          0.070423\n",
      "net                            0.069182\n",
      "deep learning                  0.068271\n",
      "gis                            0.064912\n",
      "clustering                     0.064539\n",
      "regression                     0.063979\n",
      "redshift                       0.063977\n",
      "machine learning               0.063627\n",
      "git                            0.061985\n",
      "neural network                 0.061120\n",
      "jupyter                        0.061079\n",
      "cleaning                       0.059791\n",
      "a/b testing                    0.059387\n",
      "hive                           0.057402\n",
      "data mining                    0.056821\n",
      "tf                             0.055699\n",
      "research                       0.055101\n",
      "docker                         0.054552\n",
      "unsupervised                   0.053951\n",
      "random forest                  0.053211\n",
      "supervised                     0.052698\n",
      "mysql                          0.052040\n",
      "sas                            0.051956\n",
      "logistic                       0.051801\n",
      "hypothesis                     0.051302\n",
      "php                            0.050920\n",
      "decision tree                  0.050088\n",
      "aws                            0.049929\n",
      "tensor                         0.049612\n",
      "pca                            0.049552\n",
      "dimensionality reduction       0.049426\n",
      "scikit                         0.047825\n",
      "kubernetes                     0.047081\n",
      "perl                           0.046839\n",
      "s3                             0.046672\n",
      "tensorflow                     0.046459\n",
      "sql server                     0.046449\n",
      "pandas                         0.044925\n",
      "anomaly                        0.042648\n",
      "natural language processing    0.041300\n",
      "senior                         0.040473\n",
      "pytorch                        0.039395\n",
      "etl                            0.038794\n",
      "correlation                    0.038485\n",
      "pyspark                        0.038088\n",
      "sklearn                        0.038014\n",
      "java                           0.037314\n",
      "shiny                          0.036657\n",
      "dt                             0.035754\n",
      "h2o                            0.035566\n",
      "javascript                     0.034622\n",
      "node                           0.034411\n",
      "scipy                          0.033817\n",
      "numpy                          0.033645\n",
      "keras                          0.032452\n",
      "biostatistics                  0.031406\n",
      "forecasting                    0.031307\n",
      "rvest                          0.031090\n",
      "qlik                           0.030806\n",
      "bokeh                          0.030082\n",
      "ggplot2                        0.029653\n",
      "linux                          0.029562\n",
      "machine vision                 0.028058\n",
      "azure                          0.027611\n",
      "geospatial                     0.027299\n",
      "github                         0.027137\n",
      "hyperparameter                 0.026832\n",
      "lambda                         0.026676\n",
      "snowflake                      0.026509\n",
      "salesforce                     0.026314\n",
      "experimental design            0.026037\n",
      "bioinformatics                 0.026020\n",
      "ssrs                           0.025302\n",
      "cleansing                      0.024313\n",
      "matplotlib                     0.023530\n",
      "blockchain                     0.023131\n",
      "recurrent                      0.022595\n",
      "Month                          0.022445\n",
      "Day                            0.020837\n",
      "recommender                    0.019960\n",
      "cloud                          0.019947\n",
      "nearest neighbors              0.018967\n",
      "computer vision                0.017639\n",
      "causality                      0.015315\n",
      "chatbot                        0.014826\n",
      "powerbi                        0.014284\n",
      "plotly                         0.014249\n",
      "support vector machine         0.013576\n",
      "rust                           0.013373\n",
      "xgboost                        0.011306\n",
      "text analytics                 0.011302\n",
      "flask                          0.011099\n",
      "django                         0.011001\n",
      "reinforcement                  0.009543\n",
      "elasticsearch                  0.009258\n",
      "seaborn                        0.009038\n",
      "react                          0.008196\n",
      "probability                    0.007315\n",
      "tuning                         0.007290\n",
      "theano                         0.007116\n",
      "boosting                       0.007010\n",
      "hbase                          0.006579\n",
      "angular                        0.006547\n",
      "graph                          0.005968\n",
      "time series                    0.005868\n",
      "convolutional                  0.005834\n",
      "prediction                     0.004863\n",
      "nltk                           0.004538\n",
      "spacy                          0.004313\n",
      "d3                             0.004035\n",
      "unix                           0.003137\n",
      "segmentation                   0.002742\n",
      "ocr                            0.002715\n",
      "sequencing                     0.002631\n",
      "optimizing                     0.000957\n",
      "bandit                         0.000690\n",
      "normalization                  0.000472\n",
      "Year                                NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "merged_df_corr = merged_df.drop(['Description', 'Title', 'Company Name', 'City', 'State'], axis=1)\n",
    "\n",
    "salary_column = merged_df_corr['Salary']\n",
    "correlations = merged_df_corr.corrwith(salary_column)\n",
    "\n",
    "\n",
    "sorted_correlations = correlations.abs().sort_values(ascending=False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(sorted_correlations)\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b2576",
   "metadata": {},
   "source": [
    "## Select the top 20 correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebe95862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Title</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>Year</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>python</th>\n",
       "      <th>...</th>\n",
       "      <th>dashboard</th>\n",
       "      <th>classification</th>\n",
       "      <th>recommendations</th>\n",
       "      <th>dash</th>\n",
       "      <th>visualization</th>\n",
       "      <th>scala</th>\n",
       "      <th>exploratory</th>\n",
       "      <th>deep learning</th>\n",
       "      <th>birt</th>\n",
       "      <th>net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>New York</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>90000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Carolinas HealthCare System</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NC</td>\n",
       "      <td>125000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>New York</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>125000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TV Data Scientist in Burlington, MA Our servic...</td>\n",
       "      <td>TV Data Scientist in Burlington, MA</td>\n",
       "      <td>LiveRamp</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>125000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TV Data Scientist in Burlington, MA ABOUT THIS...</td>\n",
       "      <td>TV Data Scientist in Burlington, MA</td>\n",
       "      <td>LiveRamp</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>125000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>About Us Our mission is to save lives, reduce ...</td>\n",
       "      <td>Epidemiology Investigator 2 (Hybrid Eligible)</td>\n",
       "      <td>Ohio Department of Public Safety</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>57907</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Wholesale Business Analyst - Montvale, NJ (Hyb...</td>\n",
       "      <td>Wholesale Business Analyst - Montvale, NJ (Hyb...</td>\n",
       "      <td>Flight Centre Travel Group, The Americas</td>\n",
       "      <td>Montvale</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>75000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>Senior AEM Developer | Mountain view, CA | Pho...</td>\n",
       "      <td>AEM Developer</td>\n",
       "      <td>RD SOLUTIONS INC</td>\n",
       "      <td>Iselin</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>135200</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>ABOUT ELEVATE SPORTS VENTURES:Elevate Sports V...</td>\n",
       "      <td>MBA Fall Intern, Insights</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>52000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>ABOUT ELEVATE SPORTS VENTURES: Elevate Sports ...</td>\n",
       "      <td>Fall Undergraduate Intern, Insights</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>41600</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1468 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Description  \\\n",
       "0     Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1     Senior Data Scientist - Infectious Disease and...   \n",
       "2     Senior Data Scientist, Advanced Marketing Anal...   \n",
       "3     TV Data Scientist in Burlington, MA Our servic...   \n",
       "4     TV Data Scientist in Burlington, MA ABOUT THIS...   \n",
       "...                                                 ...   \n",
       "1463  About Us Our mission is to save lives, reduce ...   \n",
       "1464  Wholesale Business Analyst - Montvale, NJ (Hyb...   \n",
       "1465  Senior AEM Developer | Mountain view, CA | Pho...   \n",
       "1466  ABOUT ELEVATE SPORTS VENTURES:Elevate Sports V...   \n",
       "1467  ABOUT ELEVATE SPORTS VENTURES: Elevate Sports ...   \n",
       "\n",
       "                                                  Title  \\\n",
       "0     Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1     Senior Data Scientist - Infectious Disease and...   \n",
       "2     Senior Data Scientist, Advanced Marketing Anal...   \n",
       "3                   TV Data Scientist in Burlington, MA   \n",
       "4                   TV Data Scientist in Burlington, MA   \n",
       "...                                                 ...   \n",
       "1463      Epidemiology Investigator 2 (Hybrid Eligible)   \n",
       "1464  Wholesale Business Analyst - Montvale, NJ (Hyb...   \n",
       "1465                                      AEM Developer   \n",
       "1466                          MBA Fall Intern, Insights   \n",
       "1467                Fall Undergraduate Intern, Insights   \n",
       "\n",
       "                                  Company Name        City    Year   Day  \\\n",
       "0                               Morgan Stanley    New York  2019.0  20.0   \n",
       "1                  Carolinas HealthCare System   Charlotte  2019.0   6.0   \n",
       "2                                      Spotify    New York  2019.0  23.0   \n",
       "3                                     LiveRamp  Burlington  2019.0   2.0   \n",
       "4                                     LiveRamp  Burlington  2019.0   2.0   \n",
       "...                                        ...         ...     ...   ...   \n",
       "1463          Ohio Department of Public Safety    Columbus  2019.0  16.0   \n",
       "1464  Flight Centre Travel Group, The Americas    Montvale  2019.0  16.0   \n",
       "1465                          RD SOLUTIONS INC      Iselin  2019.0  16.0   \n",
       "1466                                   Elevate   Charlotte  2019.0  16.0   \n",
       "1467                                   Elevate   Charlotte  2019.0  16.0   \n",
       "\n",
       "      Month           State  Salary  python  ...  dashboard  classification  \\\n",
       "0       8.0              NY   90000       1  ...          0               0   \n",
       "1       9.0              NC  125000       0  ...          0               0   \n",
       "2       8.0              NY  125000       1  ...          0               0   \n",
       "3       8.0              MA  125000       1  ...          1               0   \n",
       "4       8.0              MA  125000       1  ...          1               0   \n",
       "...     ...             ...     ...     ...  ...        ...             ...   \n",
       "1463    9.0            Ohio   57907       0  ...          2               0   \n",
       "1464    9.0              NJ   75000       0  ...          1               0   \n",
       "1465    9.0      New Jersey  135200       0  ...          0               0   \n",
       "1466    9.0  North Carolina   52000       1  ...          0               0   \n",
       "1467    9.0  North Carolina   41600       1  ...          0               0   \n",
       "\n",
       "      recommendations  dash  visualization  scala  exploratory  deep learning  \\\n",
       "0                   0     0              1      0            0              0   \n",
       "1                   0     0              1      1            0              0   \n",
       "2                   1     0              1      0            0              0   \n",
       "3                   0     1              1      0            0              0   \n",
       "4                   0     1              1      0            0              0   \n",
       "...               ...   ...            ...    ...          ...            ...   \n",
       "1463                2     2              0      0            0              0   \n",
       "1464                0     1              0      0            0              0   \n",
       "1465                1     0              0      1            0              0   \n",
       "1466                1     0              4      0            0              0   \n",
       "1467                1     0              3      0            0              0   \n",
       "\n",
       "      birt  net  \n",
       "0        0    0  \n",
       "1        0    2  \n",
       "2        0    0  \n",
       "3        0    1  \n",
       "4        0    1  \n",
       "...    ...  ...  \n",
       "1463     1    0  \n",
       "1464     1    4  \n",
       "1465     0    4  \n",
       "1466     0    1  \n",
       "1467     0    1  \n",
       "\n",
       "[1468 rows x 27 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = merged_df[['Description', 'Title', 'Company Name', 'City', 'Year', 'Day', 'Month','State','Salary'\n",
    "                      ,'python','CS_keywords','rstan' ,'sql','intern', 'optimization','spark'\n",
    "                      ,'tableau','dashboard','classification','recommendations','dash','visualization'\n",
    "                      ,'scala','exploratory','deep learning','birt', 'net']]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8de66a",
   "metadata": {},
   "source": [
    "## Push the dataset to Amazon S3 datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb242bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'EPHZHWSX7TSB851N',\n",
       "  'HostId': 'RCqdvarObC1gMyXBDxC6cVIuOF1jF3sKxHlgYop5DVzfIbiCU3NbZGmpxv4LnHVtZBuwScYZ4go=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'RCqdvarObC1gMyXBDxC6cVIuOF1jF3sKxHlgYop5DVzfIbiCU3NbZGmpxv4LnHVtZBuwScYZ4go=',\n",
       "   'x-amz-request-id': 'EPHZHWSX7TSB851N',\n",
       "   'date': 'Tue, 20 Feb 2024 23:35:55 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"61074646a03a5773f942eff0b99ff554\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"61074646a03a5773f942eff0b99ff554\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file_key = 'with_20_keywords.csv'\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "final_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3.put_object(Body = csv_buffer.getvalue(), Bucket = bucket_name, Key=output_file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae50616",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e9378",
   "metadata": {},
   "source": [
    "#### Before normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e228332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        90000\n",
      "1       125000\n",
      "2       125000\n",
      "3       125000\n",
      "4       125000\n",
      "         ...  \n",
      "1463     57907\n",
      "1464     75000\n",
      "1465    135200\n",
      "1466     52000\n",
      "1467     41600\n",
      "Name: Salary, Length: 1468, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_df['Salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f18e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "final_df.loc[:, 'Salary'] = scaler.fit_transform(final_df['Salary'].values.reshape(-1, 1))\n",
    "# If we want to reverse the normalization\n",
    "# final_df.loc[:, 'Salary'] = scaler.inverse_transform(final_df['Salary'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010944f",
   "metadata": {},
   "source": [
    "#### After normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54a54da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.140306\n",
      "1       0.233736\n",
      "2       0.233736\n",
      "3       0.233736\n",
      "4       0.233736\n",
      "          ...   \n",
      "1463    0.054635\n",
      "1464    0.100264\n",
      "1465    0.260965\n",
      "1466    0.038867\n",
      "1467    0.011105\n",
      "Name: Salary, Length: 1468, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(final_df['Salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af10ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
