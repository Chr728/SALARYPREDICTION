{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737dfc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1014efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re \n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import io\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import pearsonr\n",
    "import textdistance\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87444577",
   "metadata": {},
   "source": [
    "## Credentials for Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec4694f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = ''\n",
    "aws_secret_access_key = ''\n",
    "\n",
    "bucket_name = ''\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208713f0",
   "metadata": {},
   "source": [
    "## The list contains generic CS keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b702dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_keywords = set(['python', 'pytorch', 'sql', 'mxnet', 'mlflow', 'einstein', 'theano', 'pyspark', 'solr', 'mahout',\n",
    " 'cassandra', 'aws', 'powerpoint', 'spark', 'pig', 'sas', 'java', 'nosql', 'docker', 'salesforce', 'scala', 'c++', 'net', 'tableau', 'pandas', 'scikitlearn', 'sklearn', 'matlab', 'scala', 'keras', 'tensorflow', 'clojure',\n",
    " 'caffe', 'scipy', 'numpy', 'matplotlib', 'vba', 'spss', 'linux', 'azure', 'cloud', 'gcp', 'mongodb', 'mysql', 'oracle',\n",
    " 'redshift', 'snowflake', 'kafka', 'javascript', 'qlik', 'jupyter', 'perl', 'bigquery', 'unix', 'react',\n",
    " 'scikit', 'powerbi', 's3', 'ec2', 'lambda', 'ssrs', 'kubernetes', 'hana', 'spacy', 'tf', 'django', 'sagemaker',\n",
    " 'seaborn', 'mllib', 'github', 'git', 'elasticsearch', 'splunk', 'airflow', 'looker', 'rapidminer', 'birt', 'pentaho',\n",
    "'jquery', 'nodejs', 'd3', 'plotly', 'bokeh', 'xgboost', 'rstudio', 'shiny', 'dash', 'h20', 'h2o', 'hadoop', 'mapreduce',\n",
    " 'hive', 'cognos', 'angular', 'nltk', 'flask', 'node', 'firebase', 'bigtable', 'rust', 'php', 'cntk', 'lightgbm',\n",
    " 'kubeflow', 'rpython', 'unixlinux', 'postgressql', 'postgresql', 'postgres', 'hbase', 'dask', 'ruby', 'julia', 'tensor',\n",
    " 'dplyr','ggplot2','esquisse','bioconductor','shiny','lubridate','knitr','mlr','quanteda','dt','rcrawler','caret','rmarkdown',\n",
    " 'leaflet','janitor','ggvis','plotly','rcharts','rbokeh','broom','stringr','magrittr','slidify','rvest',\n",
    " 'rmysql','rsqlite','prophet','glmnet','text2vec','snowballc','quantmod','rstan','swirl','datasciencer', \n",
    " 'amazon web services', 'google cloud', 'sql server',\n",
    "  'cleansing', 'chatbot', 'cleaning', 'blockchain', 'causality', 'correlation', 'bandit', 'anomaly', 'kpi',\n",
    " 'dashboard', 'geospatial', 'ocr',  'pca', 'gis', 'svm', 'svd', 'tuning', 'hyperparameter', 'hypothesis',\n",
    " 'salesforcecom', 'segmentation', 'biostatistics', 'unsupervised', 'supervised', 'exploratory',\n",
    " 'recommender', 'recommendations', 'research', 'sequencing', 'probability', 'reinforcement', 'graph', 'bioinformatics',\n",
    "  'knn', 'etl', 'normalization', 'classification', 'optimizing', 'prediction', 'forecasting',\n",
    " 'clustering', 'optimization', 'visualization', 'nlp', 'c#',\n",
    " 'regression', 'logistic', 'cnn', 'glm',\n",
    " 'rnn', 'lstm', 'gbm', 'boosting', 'recurrent', 'convolutional', 'bayesian',\n",
    " 'bayes', 'random forest', 'natural language processing', 'machine learning', 'decision tree', 'deep learning', 'experimental design',\n",
    " 'time series', 'nearest neighbors', 'neural network', 'support vector machine', 'computer vision', 'machine vision', 'dimensionality reduction',\n",
    " 'text analytics',  'a/b testing', 'data mining', 'kajsadouas', 'senior','intern', 'Data', ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e0bb4",
   "metadata": {},
   "source": [
    "## Get the list of unique keywords that appear in the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5a5067d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>CS_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>90000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Carolinas HealthCare System</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Senior Data Scientist, Advanced Marketing Anal...</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1  Senior Data Scientist - Infectious Disease and...   \n",
       "2  Senior Data Scientist, Advanced Marketing Anal...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1  Senior Data Scientist - Infectious Disease and...   \n",
       "2  Senior Data Scientist, Advanced Marketing Anal...   \n",
       "\n",
       "                  Company Name       City           State  Salary    Year  \\\n",
       "0               Morgan Stanley   New York        New York   90000  2019.0   \n",
       "1  Carolinas HealthCare System  Charlotte  North Carolina  125000  2019.0   \n",
       "2                      Spotify   New York        New York  125000  2019.0   \n",
       "\n",
       "   Month   Day  CS_keywords  \n",
       "0    8.0  20.0            2  \n",
       "1    9.0   6.0            5  \n",
       "2    8.0  23.0           11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_key = 'merged_dataset.csv'\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "content = obj['Body'].read()\n",
    "merged_df = pd.read_csv(BytesIO(content), engine='python')\n",
    "display(merged_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c584245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jeremy/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jeremy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "/tmp/ipykernel_63626/948814392.py:35: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  correlation, _ = pearsonr(word_vector, merged_df['Salary'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with highest correlation with salary: [('python', 0.19886745519756654), ('work', 0.19582876337073574), ('data', 0.19295753214710162), ('experience', 0.17431311358590212), ('scientist', 0.17268792569686944), ('new', 0.15354276957001775), ('opportunities', 0.15019754403229654), ('insights', 0.14787489406646623), ('skills', 0.1468227181885829), ('problems', 0.1397344220184944), ('communication', 0.13341254386076665), ('help', 0.13216785971370518), ('build', 0.1311344348955121), ('programming', 0.12954808497310677), ('team', 0.12875060880594835), ('drive', 0.12868237014977418), ('including', 0.12748442462796286), ('passion', 0.12350654759714375), ('business', 0.12331215076141142), ('responsibilities', 0.12300449949198337), ('statistics', 0.12196279486537379), ('sql', 0.12170040785798925), ('key', 0.11976724543630891), ('ca', 0.11853650850023585), ('actionable', 0.11781130547401442), ('ability', 0.11693902421646957), ('learn', 0.1168654047039393), ('analyses', 0.11655381756896774), ('open', 0.11505980975710751), ('environment', 0.11291593563447203), ('knowledge', 0.1101997313294543), ('stakeholders', 0.10808904915367534), ('tools', 0.1080629602428131), ('director', 0.10657614750155028), ('understanding', 0.10580263247793194), ('metrics', 0.10520476550331198), ('concepts', 0.10430424285145044), ('computer', 0.10193593708838333), ('preferred', 0.10112999719625476), ('end', 0.09958616644370342), ('initiatives', 0.09895482213344196), ('qualifications', 0.09782505973217882), ('develop', 0.09734637532157757), ('believe', 0.09593464219791421), ('related', 0.09394732109591142), ('future', 0.09313623861794067), ('verbal', 0.09190373290077107), ('growth', 0.09105302169196158), ('organize', 0.0904352039815721), ('well', 0.0884329367845056), ('time', 0.08808867923590674), ('life', 0.08775154307063518), ('source', 0.08754258209120075), ('quickly', 0.08701480366879649), ('closely', 0.0864820330446309), ('tableau', 0.08575484272876671), ('plus', 0.08511128936209464), ('excel', 0.08378001067855831), ('required', 0.08355409788120002), ('phd', 0.08283276018342252), ('big', 0.08063016749311282), ('effectively', 0.08021811629835177), ('inform', 0.08006206459855167), ('throughout', 0.07903754526605489), ('hadoop', 0.07891712159349165), ('20', 0.07788874011437735), ('overview', 0.07228721812769674), ('similar', 0.07223847954693341), ('dashboards', 0.0713577940560231), ('system', 0.07129685882550077), ('transform', 0.06961522031227407), ('language', 0.06734536402462274), ('physics', 0.06701159336617629), ('highly', 0.0665544163667273), ('enjoy', 0.06609677339474185), ('50', 0.06486917119612429), ('want', 0.06485194456592576), ('monitor', 0.06357925384604597), ('public', 0.05961077163429916), ('accuracy', 0.05198350690689704), ('10', 0.051811654782066494), ('kafka', 0.04991502497539833), ('check', 0.04848828665403788), ('numpy', 0.03427149924117962), ('today', 0.03396194552026517), ('40', 0.03134866283655925)]\n"
     ]
    }
   ],
   "source": [
    "# Suppress PerformanceWarning\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame is highly fragmented.*\")\n",
    "\n",
    "# Tokenize the words and remove stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "merged_df['Description'] = merged_df['Description'].apply(lambda x: ' '.join([word.capitalize() for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
    "\n",
    "# Count word frequencies\n",
    "word_frequencies = {}\n",
    "for description in merged_df['Description']:\n",
    "    words = word_tokenize(description)\n",
    "    for word in words:\n",
    "        word = word.lower()  # Convert to lowercase for consistency\n",
    "        if word not in word_frequencies:\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "# Filter out words with frequency above 50\n",
    "frequent_words = {word: freq for word, freq in word_frequencies.items() if freq > 50}\n",
    "\n",
    "# Create bag-of-words representation\n",
    "vectorizer = CountVectorizer(vocabulary=frequent_words.keys())\n",
    "X = vectorizer.fit_transform(merged_df['Description'])\n",
    "\n",
    "# Calculate correlation\n",
    "correlations = {}\n",
    "for word in frequent_words:\n",
    "    idx = vectorizer.vocabulary_.get(word)\n",
    "    if idx is not None:\n",
    "        word_vector = X[:, idx].toarray().flatten()\n",
    "        correlation, _ = pearsonr(word_vector, merged_df['Salary'])\n",
    "        if not pd.isnull(correlation):  # Exclude words with NaN correlation\n",
    "            correlations[word] = abs(correlation)\n",
    "\n",
    "# Find redundant words based on Jaro-Winkler similarity and keep only the one with the highest correlation\n",
    "threshold = 0.7 # You can adjust this threshold as needed\n",
    "redundant_words = set()\n",
    "for word1 in correlations:\n",
    "    for word2 in correlations:\n",
    "        if word1 != word2 and textdistance.jaro_winkler.similarity(word1, word2) > threshold:\n",
    "            if correlations[word1] > correlations[word2]:\n",
    "                redundant_words.add(word2)\n",
    "            else:\n",
    "                redundant_words.add(word1)\n",
    "\n",
    "# Remove redundant words from frequent_words\n",
    "frequent_words = {word: freq for word, freq in frequent_words.items() if word not in redundant_words}\n",
    "\n",
    "# Sort the words based on their correlation with the salary column and remove redundant words\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_correlations = [(word, corr) for word, corr in sorted_correlations if word not in redundant_words]\n",
    "\n",
    "#print(\"Words with frequency above 50 and not redundant:\", len(frequent_words))\n",
    "print(\"Words with highest correlation with salary:\", sorted_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa1af1e",
   "metadata": {},
   "source": [
    "## Adding top 30 correlated keyword columns in the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd49015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add frequency columns for top 30 correlated keywords\n",
    "for word, _ in sorted_correlations[:30]:\n",
    "    merged_df[word] = merged_df['Description'].apply(lambda x: sum(textdistance.jaro_winkler.similarity(word, w) > threshold for w in word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b77956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Title', 'Description', 'Company Name', 'City', 'State', 'Salary',\n",
      "       'Year', 'Month', 'Day', 'CS_keywords', 'python', 'work', 'data',\n",
      "       'experience', 'scientist', 'new', 'opportunities', 'insights', 'skills',\n",
      "       'problems', 'communication', 'help', 'build', 'programming', 'team',\n",
      "       'drive', 'including', 'passion', 'business', 'responsibilities',\n",
      "       'statistics', 'sql', 'key', 'ca', 'actionable', 'ability', 'learn',\n",
      "       'analyses', 'open', 'environment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867ce41",
   "metadata": {},
   "source": [
    "## Validating correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9e41861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary              1.000000\n",
      "work                0.221658\n",
      "data                0.194326\n",
      "python              0.193922\n",
      "experience          0.189389\n",
      "including           0.185625\n",
      "CS_keywords         0.183533\n",
      "communication       0.174846\n",
      "actionable          0.174808\n",
      "open                0.171710\n",
      "problems            0.170969\n",
      "analyses            0.169708\n",
      "drive               0.168311\n",
      "new                 0.168280\n",
      "team                0.165778\n",
      "insights            0.163176\n",
      "scientist           0.163092\n",
      "learn               0.160551\n",
      "passion             0.157631\n",
      "help                0.153462\n",
      "skills              0.148021\n",
      "build               0.142170\n",
      "ability             0.139136\n",
      "business            0.137521\n",
      "ca                  0.136612\n",
      "statistics          0.134026\n",
      "sql                 0.128271\n",
      "opportunities       0.125180\n",
      "responsibilities    0.124953\n",
      "key                 0.124281\n",
      "programming         0.111108\n",
      "environment         0.108739\n",
      "Month               0.022666\n",
      "Day                 0.020770\n",
      "Year                     NaN\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremy/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/jeremy/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "merged_df_corr = merged_df.drop(['Description', 'Title', 'Company Name', 'City', 'State'], axis=1)\n",
    "\n",
    "salary_column = merged_df_corr['Salary']\n",
    "correlations = merged_df_corr.corrwith(salary_column)\n",
    "\n",
    "\n",
    "sorted_correlations = correlations.abs().sort_values(ascending=False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(sorted_correlations)\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "062eeedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>CS_keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>statistics</th>\n",
       "      <th>sql</th>\n",
       "      <th>key</th>\n",
       "      <th>ca</th>\n",
       "      <th>actionable</th>\n",
       "      <th>ability</th>\n",
       "      <th>learn</th>\n",
       "      <th>analyses</th>\n",
       "      <th>open</th>\n",
       "      <th>environment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Data Scientist - Cross Asset Desk Strategist T...</td>\n",
       "      <td>Morgan Stanley</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>90000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist - Infectious Disease and...</td>\n",
       "      <td>Senior Data Scientist - Infectious Disease Inf...</td>\n",
       "      <td>Carolinas HealthCare System</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>125000</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1  Senior Data Scientist - Infectious Disease and...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Data Scientist - Cross Asset Desk Strategist T...   \n",
       "1  Senior Data Scientist - Infectious Disease Inf...   \n",
       "\n",
       "                  Company Name       City           State  Salary    Year  \\\n",
       "0               Morgan Stanley   New York        New York   90000  2019.0   \n",
       "1  Carolinas HealthCare System  Charlotte  North Carolina  125000  2019.0   \n",
       "\n",
       "   Month   Day  CS_keywords  ...  statistics  sql  key  ca  actionable  \\\n",
       "0    8.0  20.0            2  ...           3    0    0   3           0   \n",
       "1    9.0   6.0            5  ...           4    0    1  13           8   \n",
       "\n",
       "   ability  learn  analyses  open  environment  \n",
       "0        2      5         8     0            1  \n",
       "1        7      5         7     4            5  \n",
       "\n",
       "[2 rows x 40 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8de66a",
   "metadata": {},
   "source": [
    "## Push the dataset to Amazon S3 datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb242bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'XE378TY1B4VZDNKX',\n",
       "  'HostId': 'LUJsb6SgJhxeIaHbaJLhbvPg70kKUQKgg3FpCFs6iLSWFkEn7DJQckD5cVsOITtYj/zqmq9wwWt81wNa+uAe8AyGP5dCJLNs',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'LUJsb6SgJhxeIaHbaJLhbvPg70kKUQKgg3FpCFs6iLSWFkEn7DJQckD5cVsOITtYj/zqmq9wwWt81wNa+uAe8AyGP5dCJLNs',\n",
       "   'x-amz-request-id': 'XE378TY1B4VZDNKX',\n",
       "   'date': 'Mon, 15 Apr 2024 18:23:36 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"f330a8478c664648d35e8954c07dc69c\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"f330a8478c664648d35e8954c07dc69c\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file_key = 'with_20_keywords.csv'\n",
    "\n",
    "csv_buffer = io.StringIO()\n",
    "merged_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3.put_object(Body = csv_buffer.getvalue(), Bucket = bucket_name, Key=output_file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae50616",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e9378",
   "metadata": {},
   "source": [
    "#### Before normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e228332",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df['Salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "final_df.loc[:, 'Salary'] = scaler.fit_transform(final_df['Salary'].values.reshape(-1, 1))\n",
    "# If we want to reverse the normalization\n",
    "# final_df.loc[:, 'Salary'] = scaler.inverse_transform(final_df['Salary'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010944f",
   "metadata": {},
   "source": [
    "#### After normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a54da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df['Salary'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
